# 0주차
# 1부. 프레임워크
## 1장. 금융 머신러닝
 - 금융에서의 머신러닝 활용
   - 1 알고리즘 트레이딩
   - 2 포트폴리오 관리 및 로보어드바이저
   - 3 이상 거래 탐지
   - 4 대출/신용카드/보험 계약 심사 (계약심사: underwriting)
   - 5 자동화와 챗봇
   - 6 위험 관리
   - 7 자산 가치 예측
   - 8 파생 상품 가격 책정
   - 9 감정 분석
   - 10 거래 결제
   - 11 돈세탁 방지
- 머신러닝의 유형
  - 지도학습(과제 기반), 비지도학습(데이터 기반), 강화학습(실수로부터의 학습)
    - 지도학습: 회귀호, 분류
    - 비지도학습: 차원축소, 군집화
    - 강화학습: 강화학습에는 분명한 답은 없음. 학습 시스템(에이전트)가 무엇을 할지 결정. 알고리즘이 경험을 통해 답을 결정한다.
    - 자연어 처리
## 2장. 머신러닝 모델 개발

  - 머신러닝에서 파이썬이 가장 지배적인 프로그래밍 언어.
  - 머신러닝에서 주로 사용하는 파이썬 패키지
    - 데이터 표현: NumPy, Pandas
    - 머신러닝, 통계분석: SciPy, Scikit-learn, StatsModels, Keras, Theano, TensorFlow
    - 데이터 시각화: Matplotlib, Seaborn
    - 패키지 관리: pip, Conda
      - 이 중 사이킷런과 케라스가 핵심 패키지. 
- 모델 개발 단계
  - 1 문제 정의 -> 2 데이터&패키지 불러오기 -> 3 탐색적 데이터 분석 ->
  - 4 데이터 준비 -> 5 모델 평가 -> 6 모델 튜닝 및 개선 -> 7 모델 결정
## 3장. 인공신경망
- 인공신경망ANN artificial neural networks
  - 딥러닝은 복잡한 인공신경망 알고리즘을 다룬다.
  - 복잡도는 모델 전체에서 정보 흐름을 나타내는 정교한 패턴에 좌우됨.
  - 딥러닝은 모종의 세계를 개념이 층층이 쌓인 계층적 구조로 표현 가능.
 - 인공신경망 구조: 뉴런, 계층, 가중치
 - 인공신경망 훈련: 순전파, 역전파, 경사하강법
 - 인공신경망 파라미터: 계층, 노드 수, 활성화함수, 손실함수, 학습률
### 3.1. 구조, 훈련, 하이퍼파라미터
  - 뉴런은 층을 이루며 복잡하게 배열됨.
  - 모델을 거쳐 얻은 결과와 에상치를 비교하는 방식으로 훈련 단계를 진행.
  - 이를 통해 데이터 내의 패턴을 인식하는 방법을 학습한다.
   - 뉴런(=인공 뉴런, 노드, 퍼셉트론)
    - 하나 이상의 입력과 출력은 단 하나.
     - 뉴런에 있는 활성화 함수가 여러 입력과 하나의 출력 간 복잡한 비선형적 함수의 매핑을 결정한다.
     - 입력(x_1, ..., x_n)을 받아서 훈련 매개변수 적용해 가중치 합(z) 생성한 후 활성화함수(f)에 전달하여 출력 계산f(z).
     - 하나의 뉴런에서 나온 출력만으로는 복잡한 계산 불가능.
     - 수직, 수평적으로 뉴런을 쌓아간다.
     - 입력층으로 입력이 오면, 입력에 직접 노출되지 않는 은닉층으로. 가장 단순한 신경망 구조는 은닉층에 뉴런이 하나 있는 형태.
     - 세 개 이상 많은 은닉층을 가지면 심층 신경망DNN이라고 한다. 더 많은 연산이 필요하다.
     - 출력층은 한 개의 값이나 벡터값을 출력함.
   - 뉴런 가중치
      - 단위 간 연결 강도를 나타냄. 입력이 출력에 미치는 영향을 수치화.
      - 가중치 0은 입력의 변화가 출력에 영향 없는 상태, 음의 가중치는 입력 증가가 출력에는 감소를 의미.
 - 훈련
  - 훈련이란 모든 가중치를 적합하게 조정해나가는 것. 순전파와 역전파 단계를 반복.
  - 순전파
   - 입력값 받고 예측값이라는 출력을 얻는 과정. 입력 받을 때는 어떠한 연산도 없음.
   - 다음 층에서는 입력값에 곱셈, 덧셈, 활성화 연산을 하고 결과를 다음 층에 전달.
   - 순차적으로 진행 후 마지막 충에서 출력값 도출.
  - 역전파
   - 예측값과 기댓값의 차이를 손실함수(비용함수) J(w)로 변환. w는 가중치.
   - 목표는 훈련셋을 가지고 손실함수를 최적화하는 것. 즉, 손실을 가능한 최소화하는 것.
   - 이 때 사용하는 방법이 경사하강법. 경사가 낮아지는 방향으로 이동하여 가장 낮은 값이 될 때까지.
   - J(w)의 경사를 찾아야 한다. p q r을 각각의 층이라 하면 J(w)=r(q(p()))이다.
   - 첫 층의 가중치의 경사는 두, 세번째 경사에 의존. 두 번째 층의 경사는 세번째 층 가중치의 경사에 의존함.
   - 마지막인 세번째 층부터 역방향 미분을 적용해 나간다. 따라서 역전파이다.
   - 모델 오차를 망을 통해 한 번에 하나의 층으로 전파해 나가며 오차만큼 가중치를 튜닝해나간다.
   - 역전파는 경사를 찾는 가장 명확하고 효율적인 방법이다.
  - 하이퍼파라미터
    - 훈련 이전 정하는 변수라 훈련 중에 바뀌지 않는 변수.
    - 하이퍼파라미터를 통해 신경망을 유연하게 하지만, 이 유연성 때문에 모델 튜닝이 어려워짐.
  - 은닉층 및 노드의 수
   - 은닉층과 노드가 많아지면 매개변수도 많아지고, 모델은 복잡한 함수 구현에 적합해짐.
   - 잘 일반화할 수 있는 훈련된 망 가지려면 은닉층과 은닉층 노드 개수 최적화가 필요.
   - 층 수가 적으면 시스템 오차 증가, 층 수가 많으면 과적합. 그러나 결정하는 확실한 방법은 없다.

     - 대규모 이미지, 음성 인식 같은 복잡한 작업은 수십개 층과 엄청 많은 훈련 데이터 필요.
     - 대부분의 문제에서는 한두개 은닉층으로 시작해서 과적합될 때까지 층의 개수 늘려나감.
     - 숨겨진 노드 수는 입출력 노드 수, 훈련 데이터 양, 모델에서의 함수 복잡도와 관련.
     - 경험적으로, 각 층 숨겨진 노드는 입출력 크기 사이의 중간값으로 정하면 됨.
     - 숨겨진 노드는 과적합 방지를 위해 입력 노드 2배를 넘기면 안 됨.
   - 학습률
     - 가중치 최적화를 위해 순전파, 역전파를 반복한다.
     - 반복마다 각 가중치에 대한 손실함수 미분을 계산하고, 미분값을 가중치에서 뺀다.
     - 얼마나 빠르게/느리게 가중치 값을 변하게 할지는 학습률이 ㄱ려정.
     - 적당한 시간 안에 수렴할 만큼 학습률은 커야 하고, 손실함수 최솟값을 찾을만큼 작아야 한다.
   - 활성화 함수
        - 기대하는 출력을 얻기 위해 입력 가중치 합을 받는 함수이다.
        - 활성화함수를 이용해 신경망은 더 복잡한 방법으로 입력 결합 가능.
        - 또한 신경망 모델과 그 출력의 관계를 더 잘 표현 가능. 어떤 뉴런을 활성화할지, 즉 어떤 정보를 다음 층에 전달할지 결정한다.
        - 활성화함수가 없다면 인공신경망은 큰 학습 능력을 잃게 된다.
            - 선형함수(항등함수)
                - 직선 방정식(f(x)=mx + c)로 나타내고 활성은 입력에 비례한다.
                - 여러 층이 있고 모두 선형적이면, 마지막 층 활성화함수는 첫 번째 층 선형 함수와 같다.
                - 선형 함수 범위는 -inf에서 +inf.
            - 시그모이드 함수
                - S자 형의 함수. 출력 범위는 0~1. 논리 활성화함수 라고도 한다.
                - f(x) = 1 / (1 + e^-x)
            - 하이퍼볼릭 탄젠트 함수(Tanh)
                - Tanh(x) = 2Sigmoid(2x) - 1
                - -1~+1까지의 출력 범위.
            - 렐루 함수 ReLU: 정류선형유닛 Rectified Linear Unit의 줄임말
                - f(x) = max(x, 0)
                - 양수면 입력값을 그대로 전달, 음수면 0을 전달. 함수의 단순성 때문에 많이 활용.
        - 활성화함수를 고르는 방법은 없다.
        - 단지 문제 속성과 모델로 표현되는 관계에 의존할 뿐.
        - 여러 다른 활성화 함수를 적용해보고 더 빨리 수렴하고 효율적 훈련을 보여주는 함수로 선택할 수도 있다.
        - 출력층에서의 활성화함수는 대부분 모델로 표현되는 문제 유형으로 결정됨.
    - 비용 함수(손실 함수)
        - 인공신경망 성능의 정도와 얼마나 실험 데이터에 적합한지를 측정한다.
            - 평균 제곱 오차 MSE
                - 회귀형 문제에 우선적으로 적용. 연속값을 출력한다.
                - 예측과 실제 차이에 제곱하여 평균 낸 것.
            - 교차 엔트로피 (로그 손실)
                - 분류형 문제에 우선적으로 적용.
                - 0~1 사이의 확률값을 출력된다.
                - 예측되는 확률이 실제 분류에서 멀어지면 교차 엔트로피 손실이 커진다. 완벽한 모델은 0의 교차 엔트로피를 갖는 것.
    - 최적화 알고리즘
        - 손실함수를 최소화하는 방향으로 가중치 매개변수를 갱신한다.
        - 비용함수는 최적화 알고리즘에 지역을 안내하는 역할, 전역 최솟값에 도달하기 위해 올바른 방향으로 가고 있는지 알려주는 것.
            - 모멘텀
                - 이전 경사를 현재 단계에서 참조함.
                - 가중치의 이전 갱신이 현재 갱신과 같은 방향으로 간다면(모멘텀이 붙는다면) 더 큰 단계를 선택.
                - 경사 방향이 서로 상반되면 더 작은 단계를 선택
                - 골짜기에 공을 굴린다고 생각했을 때 골짜기 바닥에 가까워질수록 모멘텀이 붙는다는 것.
            - 에이다그레이드 (조정적 경사 알고리즘) Ada Grad
                - 학습률을 매개변수에 맞게 변화시킴. 자주 발생하는 특성들과 관련된 매개변수는 작게 갱신하고 드물게 발생하는 특성들과 관련된 매개변수는 크게 갱신함.
            - 알엠에스프랍 (RMS Prop)
                - 제곱 평균 제곱근 전파의 줄임말.
                - 학습률이 자동으로 변하고 각 매개변수의 학습률이 다르다.
            - 아담 (조정적 순간 예측)
                - 에이다그레이드와 알엠에스프랍 알고리즘을 결합해 최적화를 수행함. 인기가 가장 많은 경사하강 최적화 알고리즘임.
         
        - 에폭
            - 전체 훈련 데이터셋에 대해 신경망을 갱산하는 주기를 에폭이라 함.
            - 데이터 크기와 계산적 제한에 따라 수십, 수백, 수천 번의 에폭으로 신경망을 훈련함.
        - 배치 크기
            - 정방향/역방향 전달로 훈련하는 예제의 횟수.
            - 배치 크기 32는 모델 가중치 갱신 전에 훈련셋으로부터 32개 샘플 사용해 오차 경사를 예측한다는 뜻.
            - 배치 크기가 클수록 메모리 공간 많이 필요.
     
### 3.2. 인공신경망 모델 생성
- 케라스 및 머신러닝 패키지 설치
    - 사용자 친화적인 딥러닝 파이썬 라이브러리.
    - 텐서플로와 테아노 같은 복잡한 수치 계산 엔진을 감싸는 wrapper에 불과하다.
    - 그래서 먼저 텐서플로와 테아노를 설치해야 함.
```python
# 케라스의 패키지 가져오기
from Keras.models import Sequential
from Keras.layers import Dense
import numpy as np

# 데이터 불러오기
# 넘파이 모듈로 임의의 데이터와 분류 생성하기.
# (1000, 10) 크기 데이터 정렬을 먼저 만들고, 0과 1로 이루어진 (1000, 1)  크기의 분류 정렬을 만든다.
data = np.random.random((1000, 10))
Y = np.random.randint(2, size=(1000, 1))
model = Sequential()
```
- 모델 생성 - 신경망 구조
    - 선형적으로 층을 쌓는 케라스 시퀀셜 모델 사용.
    - 시퀀셜 모델 생성하고, 망 구조 완성될 때까지 한 번에 하나씩 층을 추가해 나감.
        - 입력층 입력 개수가 적당한지 가장 먼저 확인.
        - 첫 번째 층 생성할 때 입력 개수 지정.
        - 그 다음 인수 input_dim을 사용해 입력층을 다루고 있음을 나타내기 위해 밀집층dense layer 혹은 완전 연결층을 선택한다.
        - add() 함수로 모델에 층 하나 추가하고 각 층의 노드 수를 지정한다.
        - 출력층으로 또 다른 밀집층 하나 추가한다.
    - 구현하려는 모델 구조는 다음과 같음.
        - 모델에는 10개의 변수가 있다. input_dim=10 인수
        - 첫 은닉층에는 노드 32개, relu 활성화함수.
        - 두 번째 은닉층은 노드 32개, relu 활성화 함수
        - 출력층은 노드 1개, sigmoid 활성화 함수.
```python
model = Sequential()
model.add(Dense(32, input_dim=10, activation= 'relu'))
model.add(Dense(32, activation = 'relu' ))
model.add(Dense(1, activation = 'sigmoid'))
```
- 모델 컴파일
    - compile() 함수로 모델 컴파일 가능.
    - 모델 컴파일이란, 테아노나 텐서플로 패키지에 있는 수치 계산 라이브러리 이용하겠다는 것.
    - 컴파일에서는 신경망 훈련에 필요한 추가 속성을 지정하는 것이 중요.
    - 하나의 신경망 훈련이라는 것은, 문제에 대해 실제 예측하는 가중치의 최적 집합을 찾는 과정이다.
    - 따라서 가중치 집합 평가를 위한 손실함수, 신경망에 대한 다른 가중치 찾으려는 최적화 알고리즘, 훈련 중 필요한 매크릭 지정해야 함.
        - 다음 예제는 cross-entropy 손실 함수 사용. 케라스에서는 binary_crossentropy이다.
        - 아담 최적화 알고리즘 사용하고, 분류형 문제이기 때문에 매트릭으로 분류 정확도도 출력하는 코드이다.
```python
model.compile(loss= 'binary_corssentropy', optimizer = 'adam' , \
    metrics = ['accuracy'])
```
- 모델 적합화
    - fit() 함수를 호출하여 모델 훈련하거나 데이터에 적합화 할 수 있다.
    - 훈련 과정은 nb_epoch를 이용해 지정된 반복 횟수(에포크) 동안 실행됨.
    - 신경망에서 가중치가 갱신되기 전에 인수 batch_size를 이용해 배치 크기 정할 수 있음.
    - 이 변수들은 시행착오를 통해 실험적으로 정할 수 있다.
```python
model.fit(data, Y, nb_epoch=10, batch_size=32)
```
- 모델 평가
    - evaluation() 함수로 훈련셋에 대해 모델 평가 가능.
    - 이 함수는 각 입력, 출력 쌍에 대해 예측하고, 평균손실, 정확도 같이 설정된 매트릭을 포함해 점수를 기록한다.
```python
scores = model.evaluate(X_test, Y_test)
print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
```
- 더 빠른 인공 신경망 모델 실행: GPU와 클라우드 서비스
    - 거대한 계산 능력이 필요
        - CPU는 로컬 머신에서 명령어 처리와 실행을 책임짐.
        - CPU는 코어가 제한적이고 일을 순차적으로 처리하여 딥러닝 훈련에 필요한 행렬 계산을 신속하게 처리하지 못함.
        - 3가지 방법이 있다.
            - GPU가 있는 컴퓨터에서 실행
            - 캐글 커널 또는 구글 코랩에서 실행
            - AWS 이용
- 그래픽 처리 장치 GPU
    - 수천 개 스레드를 동시에 처리할 수 있는 수백 개의 코어로 구성 됨.
    - 특히 복잡한 행렬 연산 처리에 적합함.
- 클라우드 서비스
    - 캐글: 구글이 소유한 데이터 공학 웹사이트. 주피터 서비스를 제공하는데 캐글 커널이라고 함.
        - 무료 서비스이고 흔히 사용하는 패키지가 설치되어 있다.
        - 커널을 캐글 데이터 셋과 연결하거나, 진행 과정 중 새로운 데이터 셋 업로드도 가능함.
    - 구글 코랩: 주피터 노트북 환경에서 GPU 무료로 사용 가능.
    - 아마존 웹 서비스: ASW 딥러닝은 어떤 규모든 클라우드에서 딥러닝 가속화 가능한 하부구조 제공.
        - AWS 서버 인스턴스를 빠르게 실행 가능.
        - 인기 있는 딥러닝 프레임워크가 이미 설치되어 있고, 사용자 정의 인공지능 모델을 훈련하고 새로운 알고리즘 실험하며
        - 새로운 기술, 방법을 배우는데 필요한 인터페이스를 제공함.
        - AWS 서버는 캐글 커널보다 더 오래 실행 가능하다. 대규모 프로젝트라면 AWS를 추천.
- 맺음말
    - 인공신경망은 머신러닝 모든 유형에 사용되는 알고리즘의 집합체.
    - 동물 뉴런과 뉴런층이 있는 생물학적 신경망에 영감을 받음.
    - 다수의 층이 있다면 심층 신경망.
## 4장. 지도학습: 모델 및 개념
- 지도학습
    - 선택된 알고리즘이 주어진 입력값을 이용해 타겟을 적합화하는 머신러닝 영역이다.
    - 참값(레이블)이 포함된 훈련셋이 제공되고, 규칙을 학습해 나간다.
    - 새로운 관찰(입력)에 대한 참값을 예측하기 위해서.
    - 회귀 알고리즘과 분류 알고리즘으로 나뉜다.
        - 회귀 알고리즘은 무한의 해결값(연속적 집합)을 가진 문제의 결괏값을 추정.
        - 분류 알고리즘은 확률 기반으로, 출력은 분류값이지만, 해당 분류에 속할 가장 높은 확률을 찾음.
    - 지도학습 모델은 금융 분야 머신러닝에서 가장 많이 사용하는 대표 분야.
    - 알고리즘 거래에도 대부분 지도학습을 기반으로.
    - 효율적 훈련이 가능하고, 이상치 금융 데이터에 비교적 안정적이고, 금융 이론과 강한 연관성이 있기 때문.
    - 회귀
        - 학계와 산업계 연구자들은 자산가치 산정 모델 개발을 위해 회귀 기반 알고리즘을 연구해옴.
        - 기간대비 수익 예측하고, 자산 수익을 이끄는 중대한 요소 찾는데 알고리즘을 사용함.
        -  회귀 기반 지도학습은 포트폴리오 관리 및 파생 상품 가격 책정에 활용된 사례가 많음.
    - 분류
        - 분류 기반 알고리즘은 분류적 결과 도출하거나 예측하는데 사용.
        - 사기 감지, 채무 불이행 예측, 신용점수, 자산가치 동향 에측, 매수/매도 추천, 포트폴리오 관리, 가치 산정 등
### 지도학습 모델: 개념
- 분류와 회귀는 다르다.
    -  분류: 이산형 참값(레이블)을 예측
    - 회귀: 연속형 값을 예측.
        - 그러나 두 문제는 비슷하다.
        - 일부 모델은 수정 조금만 하면 분류, 회귀 모두에 사용 가능.
        - 선형 모델, 로지스틱 회귀는 둘 다 사용하지는 못한다.
- 지도학습
    - 회귀모델
        - 선형회귀, 정규화 회귀, K-최근접 이웃, 결정트리 회귀(CART), 서포트 벡터 회귀
        - 앙상블 부스팅: 에이다부스트, 경사부스팅 방법
        - 앙상블 배깅: 랜덤포레스트 방법, 엑스트라 트리
        - 인공 신경망: 인공 신경망(딥러닝 포함)
    - 분류모델
        - 로지스틱 회귀, 선형 판별 분석, K-최근접 이웃, 결정 트리 분류기, 서포트 벡터 분류기
        - 앙상블 부스팅: 에이다부스트, 경사부스팅 방법
        - 앙상블 배깅: 랜덤포레스트 방법, 엑스트라 트리
        - 인공 신경망: 인공 신경망(딥러닝 포함)
- 금융에서는 같은 시계열 동안 미래 값 예측을 위해 이전에 관찰된 데이터에서 신호를 추출하는 모델에 중점을 둔다.
- 시계열 모델은 연속 출력을 예측하고 지도 회귀 모델과 더 잘 정렬되며, 지도 회귀를 다루는 5장에서 별도로 다룰 것.
1. 선형 회귀(최소 제곱 회귀, OLS 회귀)
    - 입력변수 x와 단일 출력 변수 y 간 선형 관계가 있다고 가정함.
    - beta_0은 인터셉트(y절편)이고, beta_i 는 회귀계수이다.
    - 모델 훈련은 비용함수(손실함수) 최소화하는 방식으로 모델 매개변수를 도출한다.
    - 비용함수(손실함수) 정의하기
        - 비용함수는 모델 예측이 얼마나 부정확한지 측정.
        - RSS(잔차 제곱합)은 실젯값과 예측값 차의 제곱의 합이다.
    - 비용을 최소화하는 매개변수
        - 음의 수를 피하고 더 큰 차잇값에 가중 손실을 주기 위해서 제곱을 한다. 그 값들을 더하고 평균을 구한다.
        - 이 값이 데이터가 하나의 선에 얼마나 잘 적합하는지를 나타내는 척도가 된다.
    - 격자 탐색
        - 가능한 하이퍼파리미터 조합을 모두 생성하고, 그 조합들로 하나씩 모델 훈련하는 것.
        - 하이퍼파라미터는 모델의 외적 특성이고, 모델 세팅으로 생각할 수 있으나, 모델의 매개변수 기반으로 추정하지는 않는다.
        - 격자 탐색을 통해 하이퍼파라미터가 조절된다.
        - 격자 내에서 최적의 매개변수를 찾아내고 만다.
        - 단점은 매개변수가 추가되거나 고려되는 값이 클수록 격자 크기가 기하급수적으로 증가.
        - 사이킷런 패키지의 model_selection 모듈의 GridSearchCV 클래스로 검증 가능.
            - 첫 단계에서는 모델 객체를 생성.
                - 그 다음, 키워드가 하이퍼파리미터이고 값이 테스트할 매개변수 세팅을 나열하는 딕셔너리 정의.
                - 선형회귀는 fit_intercept이고, 부울 변수로 이 모델에 인터셉트 계산할지 여부를 결정.
                - False는 인터셉트 계산하지 않음.
            - 두 번째 단계는 GridSearchCV 객체를 생성하고, 초기화를 위해 추정 객체와 매개변수 격자, 점수화 방법, 교차 검증 선택을 제공함.
                - 교차 검증은 리샘플링 절차로 머신러닝 모델을 평가하는데 사용됨.
                - 점수화 매개변수는 모델의 검증 매트릭임.
```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, Y)

# 격자 탐색
model = LinearRegression()
param_grid = {'fit_intercept': [True, False]}

grid = GridSearchCV(estimator = model, param_grid = param_grid, scoring = 'r2', cv=kfold)
grid_result = grid.fit(X, Y)
```
- 장단점
    - 이해하고 해석하기 쉽다
    - 예측되는 변수와 예측 변수 관계가 비선형이면 올바르게 작동하지 않을 수도.
    - 과적합에 취약하고, 특성이 많은 경우 무관한 특성을 효과적으로 처리하지 못할수도.
    - 데이터가 어떠한 가정, 예를 들면 다중공선성이 없다는 가정을 따라야 함.
2. 정규화 회귀
- 선형 회귀에 독립변수가 많으면, 계수가 부실하게 결정되고 모델이 과적합될 수도. 이를 과적합 또는 고분산이라고 함.
- 과적합 조절의 일반적인 방법은 정규화.
    - 손실함수에 패널티 항을 추가해 계수가 큰 값에 이르는 것을 억제하는 것.
    - 정규화는 예측 정확도와 해석이 더 우수한 모델 만들기 위해 모델 매개변수 영향을 (0에 가깝게) 축소하는 패널티 매커니즘임.
    - 2가지 이점.
        - 예측 정확도
            - 테스트 셋에 더 잘 일반화 된다는 것. 매개변수가 너무 많은 모델은 노이즈에 적합화 되는 경향.
            - 일부 계수를 0에 근접시켜서 복잡한 모델(과도한 치우침)을 줄이는 저분산 모델로 적합화 가능.
        - 해석
            - 예측 변수가 많으면 결과 큰 그림 해석하거나 전달하기 복잡할수도.
    - 정규화 방법
        - L1 정규화 (라쏘 회귀)
            - 선형 회귀 비용 함수 RSS에 있는 계수의 절댓값을 더한다.
            - 계수가 0의 값으로 진행된다. 즉, 일부 특성이 출력 평가에서 완전히 무시됨.
            - 정규화 매개변수 lambda 값이 더 클수록 더 많은 특성이 0의 값으로 수렴함.
            - 일부 특성 완전히 제거하고 예측 계수의 하위셋을 갖게 되며, 결과적으로 모델 복잡도 감소.
            - 따라서 과적합을 줄이고 특성 선택에도 유용한 방법.
            - 예측 계수가 0의 값으로 수렴하지 않는다는 것은 그 예측 계수가 중요하다는 의미이기 때문.
                - 이를 바탕으로 특성 선택(희소 선택)이 가능.
                - lambda의 값을 0으로 하여 기본선형회귀식을 얻을 수도.
        - L2 정규화 (릿지 회귀)
            - RSS에 있는 계수의 제곱을 더한다.
            - 계수에 제약을 둔다.
            - 패널티 항(lambda)는 계수를 정규화하는데, 계수가 큰 값을 취하면 최적함수에 패널티를 줌.
            - 계수를 작게 하면 더 작은 분산과 오차값을 유도함.
            - 릿지 회귀는 모델 복잡도는 줄이지만 변수 개수는 줄이지 않는다. 다만 변수 영향을 줄일 뿐.
            - 람다가 0에 근접할 때 비용함수는 선형회귀 비용함수에 가까워짐.
            - 따라서 특성에 대한 제약이 낮아지면 (낮은 람다), 모델은 선형회귀모델에 더 가까워짐.
        - 엘라스틱 넷
            - 정규화 항을 모델에 더하는데, L1, L2 정규화를 합한 것이 된다.
            - lambda 값을 정할 뿐 아니라, 알파 매개변수를 조절할 수 있다.
            - alpha가 0이면 릿지, 1이면 라쏘에 해당한다.
            - alpha를 0~1 사이로 조절하면서 엘라스틱 넷을 최적화 함.
            - 일부 계수를 줄이고 희소 선택에 대한 일부 계수를 0으로 한다.
    - 모든 정규화 회귀에서 lambda는 핵심 매개변수로 파이썬에서 격자 탐색하는 동안 조절됨.
    - 엘라스틱 넷에서는 alpha가 추가 매개변수로 조절됨.
```python
# L1 라쏘
from sklearn.linear_model import Lasso
model = Lasso()
model.fit(X, Y)

# L2 릿지
from sklearn.linear_model import Ridge
model = Ridge()
model.fit(X, Y)

# 엘라스틱 넷
from sklearn.linear_model import ElasticNet
model = ElasticNet()
model.fit(X, Y)
```
3. 로지스틱 회귀 logistic regression
- 분류 문제에 가장 널리 사용되는 알고리즘.
- 출력 클래스의 확률 분포를 모델링 함.
- x에 대한 선형 함수가 가장 많이 사용 됨. 출력 확률은 0~1이고 합은 1.
- sigmoid 함수를 적용 해 0~1 사이의 확률을 출력하도록 선형 회귀를 변형할 수 있다.
    - 입력 데이터에 각 행에는 beta 계수가 포함됨.
    - 이 계수는 훈련데이터로 학습한다.
- 비용함수는 실젯값이 0일 때 얼마나 자주 1로 예측했는지 (그 역도 포함) 측정값으로 나타냄.
- 최대우도측정 MLE 같은 기술을 사용해 기본 클래스는 1에 가까운 값, 다른 클래스는 0에 가까운 값을 에측한다.
- 하이퍼파라미터
    -  정규화 (사이킷런의 penalty)
        -  L1, L2, 엘라스틱넷 정규화 적용 가능. 사이킷런 라이브러리 내 값은 l1, l2, elasticnet이다.
    -  정규화 강도(사이킷런의 C)
        -  패널티 매개변수의 적정 값은 100, 10, 1, 0.1, 0.01 이다.
-  장단점
    -  구현하기 쉽고, 해석성 좋고, 선형적으로 분리된 클래스에서 잘 작동.
    -  모델 출력은 확률로 나타내고, 순위 정하는데도 유용.
    -  모델은 하이퍼파리미터 개수가 적다.
    -  과적합 위험 가능성있지만, 정규화를 통해 문제 해결 가능.
    -  반면, 특성의 수가 커지면 과적합되는 가능성.
    -  선형함수만 학습 가능하고 특성과 목표 변수의 관계가 복잡할 경우 덜 적합함.
```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X, Y)
```
4. 서포트 벡터 머신 SVM
- 마진을 최대화 하는 것.
    - 마진은 분리 초평면(혹은 결정선)과 이 초평면 가장 가까이에 있는 샘플 사이의 거리로 정의됨.
    - 마진은 마진 영역 중앙선에서 가장 가까운 포인트까지의 수직거리로 계산.
    - 모든 데이터 포인트에 대해 균일하게 구분짓는 최대 마진 영역을 계산하는 것.
    - 실제 데이터는 깔끔하게 초평면으로 완벽하게 구분할 수는 없다.
        - 그래서 클래스 구분 중앙선 마진의 최대화하는 조건을 완화해야 함.
        - 그러면 일부 데이터 포인트가 중앙선을 침범하게 된다.
        - 각 차원에 꾸불꾸불한 마진 영역을 허용하는 추가적인 계수집합이 도입되고, 모든 차원에 꾸불꾸불한 정도를 나타내는 C라는 튜닝 매개변수가 도입됨.
        - C가 클수록 초평면이 더 많이 침범됨.
    - 일부 경우 초평면 혹은 선형결정영역을 찾지 못할 수도. 이 때는 커널을 사용.
        - 커널은 SVM 알고리즘이 많은 데이터를 쉽게 처리할 수 있도록 입력 데이터를 변환하는 것.
        - 커널 사용해 원래 데이터를 고차원에 투영하여 데이터를 더 잘 분류함.
    - SVM은 회귀, 분류 모두 사용됨.
        - 본래의 최적화 문제를 두 개의 문제로 나눔으로써 가능한 것이다.
        - 회귀문제에 대해서는 목표가 반대이다.
            - 마진 침범 제한하면서 두 클래스 사이의 마진 영역을 최대한 넓히는 대신, 가능한 많은 점을 마진 영역에 맞춰지게 한다.
            - 그리고 마진 영역 넓이는 하이퍼파라미터로 튜닝한다.
    - 하이퍼파리미터
        - 커널 (사이킷런의 kernel)
            - 커널 선택은 입력변수 투영 방식을 결정. 선형 커널과 RBF가 가장 보편적.
        - 패널티 (사이킷런의 C)
            - SVM 최적화에서 훈련 예제의 오분류를 얼만큼 회피할지 정도를 나타냄.
            - 이 값이 크면 최적화는 마진이 작은 초평면을 선택함. 로그 스케일로 10~1000 사이 값이 적당하다.
- 장단점
    - 과적합에 안정적. 고차원 영역일수록 좋다.
    - 비선형 관계를 다루는 커널이 다양하고 데이터 분포도를 요하지 않는 장점.
    - 반면, 학습에 비효율적이며 메모리 많이 요구.
        - 큰 데이터셋에서 좋지 않다.
    - 데이터의 스케일링 필요.
    - 하이퍼파라미터가 많으나 그 의미가 직관적이지 않다는 단점.
```python
# 회귀
from sklearn.svm import SVR
model = SVR()
model.fit(X, Y)

# 분류
from sklearn.svm import SVC
model = SVC()
model.fit(X, Y)
```
5. K-최근접 이웃

6. 선형 판별 분석

7. 분류 트리와 회귀 트리

8. 앙상블 모델

9. 인공신경망 모델

### 모델 성능 (p.95)
1. 과적합과 과소적합

2. 교차 검증

3. 평가 메트릭

### 모델 선택 (p.102)
1. 모델 선택 시 고려할 요소|

2. 모델 균형















