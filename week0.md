# 0주차
# 1부. 프레임워크
## 1장. 금융 머신러닝
 - 금융에서의 머신러닝 활용
   - 1 알고리즘 트레이딩
   - 2 포트폴리오 관리 및 로보어드바이저
   - 3 이상 거래 탐지
   - 4 대출/신용카드/보험 계약 심사 (계약심사: underwriting)
   - 5 자동화와 챗봇
   - 6 위험 관리
   - 7 자산 가치 예측
   - 8 파생 상품 가격 책정
   - 9 감정 분석
   - 10 거래 결제
   - 11 돈세탁 방지
- 머신러닝의 유형
  - 지도학습(과제 기반), 비지도학습(데이터 기반), 강화학습(실수로부터의 학습)
    - 지도학습: 회귀호, 분류
    - 비지도학습: 차원축소, 군집화
    - 강화학습: 강화학습에는 분명한 답은 없음. 학습 시스템(에이전트)가 무엇을 할지 결정. 알고리즘이 경험을 통해 답을 결정한다.
    - 자연어 처리
## 2장. 머신러닝 모델 개발

  - 머신러닝에서 파이썬이 가장 지배적인 프로그래밍 언어.
  - 머신러닝에서 주로 사용하는 파이썬 패키지
    - 데이터 표현: NumPy, Pandas
    - 머신러닝, 통계분석: SciPy, Scikit-learn, StatsModels, Keras, Theano, TensorFlow
    - 데이터 시각화: Matplotlib, Seaborn
    - 패키지 관리: pip, Conda
      - 이 중 사이킷런과 케라스가 핵심 패키지. 
- 모델 개발 단계
  - 1 문제 정의 -> 2 데이터&패키지 불러오기 -> 3 탐색적 데이터 분석 ->
  - 4 데이터 준비 -> 5 모델 평가 -> 6 모델 튜닝 및 개선 -> 7 모델 결정
## 3장. 인공신경망
- 인공신경망ANN artificial neural networks
  - 딥러닝은 복잡한 인공신경망 알고리즘을 다룬다.
  - 복잡도는 모델 전체에서 정보 흐름을 나타내는 정교한 패턴에 좌우됨.
  - 딥러닝은 모종의 세계를 개념이 층층이 쌓인 계층적 구조로 표현 가능.
 - 인공신경망 구조: 뉴런, 계층, 가중치
 - 인공신경망 훈련: 순전파, 역전파, 경사하강법
 - 인공신경망 파라미터: 계층, 노드 수, 활성화함수, 손실함수, 학습률
### 3.1. 구조, 훈련, 하이퍼파라미터
  - 뉴런은 층을 이루며 복잡하게 배열됨.
  - 모델을 거쳐 얻은 결과와 에상치를 비교하는 방식으로 훈련 단계를 진행.
  - 이를 통해 데이터 내의 패턴을 인식하는 방법을 학습한다.
   - 뉴런(=인공 뉴런, 노드, 퍼셉트론)
    - 하나 이상의 입력과 출력은 단 하나.
     - 뉴런에 있는 활성화 함수가 여러 입력과 하나의 출력 간 복잡한 비선형적 함수의 매핑을 결정한다.
     - 입력(x_1, ..., x_n)을 받아서 훈련 매개변수 적용해 가중치 합(z) 생성한 후 활성화함수(f)에 전달하여 출력 계산f(z).
     - 하나의 뉴런에서 나온 출력만으로는 복잡한 계산 불가능.
     - 수직, 수평적으로 뉴런을 쌓아간다.
     - 입력층으로 입력이 오면, 입력에 직접 노출되지 않는 은닉층으로. 가장 단순한 신경망 구조는 은닉층에 뉴런이 하나 있는 형태.
     - 세 개 이상 많은 은닉층을 가지면 심층 신경망DNN이라고 한다. 더 많은 연산이 필요하다.
     - 출력층은 한 개의 값이나 벡터값을 출력함.
   - 뉴런 가중치
      - 단위 간 연결 강도를 나타냄. 입력이 출력에 미치는 영향을 수치화.
      - 가중치 0은 입력의 변화가 출력에 영향 없는 상태, 음의 가중치는 입력 증가가 출력에는 감소를 의미.
 - 훈련
  - 훈련이란 모든 가중치를 적합하게 조정해나가는 것. 순전파와 역전파 단계를 반복.
  - 순전파
   - 입력값 받고 예측값이라는 출력을 얻는 과정. 입력 받을 때는 어떠한 연산도 없음.
   - 다음 층에서는 입력값에 곱셈, 덧셈, 활성화 연산을 하고 결과를 다음 층에 전달.
   - 순차적으로 진행 후 마지막 충에서 출력값 도출.
  - 역전파
   - 예측값과 기댓값의 차이를 손실함수(비용함수) J(w)로 변환. w는 가중치.
   - 목표는 훈련셋을 가지고 손실함수를 최적화하는 것. 즉, 손실을 가능한 최소화하는 것.
   - 이 때 사용하는 방법이 경사하강법. 경사가 낮아지는 방향으로 이동하여 가장 낮은 값이 될 때까지.
   - J(w)의 경사를 찾아야 한다. p q r을 각각의 층이라 하면 J(w)=r(q(p()))이다.
   - 첫 층의 가중치의 경사는 두, 세번째 경사에 의존. 두 번째 층의 경사는 세번째 층 가중치의 경사에 의존함.
   - 마지막인 세번째 층부터 역방향 미분을 적용해 나간다. 따라서 역전파이다.
   - 모델 오차를 망을 통해 한 번에 하나의 층으로 전파해 나가며 오차만큼 가중치를 튜닝해나간다.
   - 역전파는 경사를 찾는 가장 명확하고 효율적인 방법이다.
  - 하이퍼파라미터
    - 훈련 이전 정하는 변수라 훈련 중에 바뀌지 않는 변수.
    - 하이퍼파라미터를 통해 신경망을 유연하게 하지만, 이 유연성 때문에 모델 튜닝이 어려워짐.
  - 은닉층 및 노드의 수
   - 은닉층과 노드가 많아지면 매개변수도 많아지고, 모델은 복잡한 함수 구현에 적합해짐.
   - 잘 일반화할 수 있는 훈련된 망 가지려면 은닉층과 은닉층 노드 개수 최적화가 필요.
   - 층 수가 적으면 시스템 오차 증가, 층 수가 많으면 과적합. 그러나 결정하는 확실한 방법은 없다.

     - 대규모 이미지, 음성 인식 같은 복잡한 작업은 수십개 층과 엄청 많은 훈련 데이터 필요.
     - 대부분의 문제에서는 한두개 은닉층으로 시작해서 과적합될 때까지 층의 개수 늘려나감.
     - 숨겨진 노드 수는 입출력 노드 수, 훈련 데이터 양, 모델에서의 함수 복잡도와 관련.
     - 경험적으로, 각 층 숨겨진 노드는 입출력 크기 사이의 중간값으로 정하면 됨.
     - 숨겨진 노드는 과적합 방지를 위해 입력 노드 2배를 넘기면 안 됨.
   - 학습률
     - 가중치 최적화를 위해 순전파, 역전파를 반복한다.
     - 반복마다 각 가중치에 대한 손실함수 미분을 계산하고, 미분값을 가중치에서 뺀다.
     - 얼마나 빠르게/느리게 가중치 값을 변하게 할지는 학습률이 ㄱ려정.
     - 적당한 시간 안에 수렴할 만큼 학습률은 커야 하고, 손실함수 최솟값을 찾을만큼 작아야 한다.
   - 활성화 함수
        - 기대하는 출력을 얻기 위해 입력 가중치 합을 받는 함수이다.
        - 활성화함수를 이용해 신경망은 더 복잡한 방법으로 입력 결합 가능.
        - 또한 신경망 모델과 그 출력의 관계를 더 잘 표현 가능. 어떤 뉴런을 활성화할지, 즉 어떤 정보를 다음 층에 전달할지 결정한다.
        - 활성화함수가 없다면 인공신경망은 큰 학습 능력을 잃게 된다.
            - 선형함수(항등함수)
                - 직선 방정식(f(x)=mx + c)로 나타내고 활성은 입력에 비례한다.
                - 여러 층이 있고 모두 선형적이면, 마지막 층 활성화함수는 첫 번째 층 선형 함수와 같다.
                - 선형 함수 범위는 -inf에서 +inf.
            - 시그모이드 함수
                - S자 형의 함수. 출력 범위는 0~1. 논리 활성화함수 라고도 한다.
                - f(x) = 1 / (1 + e^-x)
            - 하이퍼볼릭 탄젠트 함수(Tanh)
                - Tanh(x) = 2Sigmoid(2x) - 1
                - -1~+1까지의 출력 범위.
            - 렐루 함수 ReLU: 정류선형유닛 Rectified Linear Unit의 줄임말
                - f(x) = max(x, 0)
                - 양수면 입력값을 그대로 전달, 음수면 0을 전달. 함수의 단순성 때문에 많이 활용.
        - 활성화함수를 고르는 방법은 없다.
        - 단지 문제 속성과 모델로 표현되는 관계에 의존할 뿐.
        - 여러 다른 활성화 함수를 적용해보고 더 빨리 수렴하고 효율적 훈련을 보여주는 함수로 선택할 수도 있다.
        - 출력층에서의 활성화함수는 대부분 모델로 표현되는 문제 유형으로 결정됨.
    - 비용 함수(손실 함수)
        - 인공신경망 성능의 정도와 얼마나 실험 데이터에 적합한지를 측정한다.
            - 평균 제곱 오차 MSE
                - 회귀형 문제에 우선적으로 적용. 연속값을 출력한다.
                - 예측과 실제 차이에 제곱하여 평균 낸 것.
            - 교차 엔트로피 (로그 손실)
                - 분류형 문제에 우선적으로 적용.
                - 0~1 사이의 확률값을 출력된다.
                - 예측되는 확률이 실제 분류에서 멀어지면 교차 엔트로피 손실이 커진다. 완벽한 모델은 0의 교차 엔트로피를 갖는 것.
    - 최적화 알고리즘
        - 손실함수를 최소화하는 방향으로 가중치 매개변수를 갱신한다.
        - 비용함수는 최적화 알고리즘에 지역을 안내하는 역할, 전역 최솟값에 도달하기 위해 올바른 방향으로 가고 있는지 알려주는 것.
            - 모멘텀
                - 이전 경사를 현재 단계에서 참조함.
                - 가중치의 이전 갱신이 현재 갱신과 같은 방향으로 간다면(모멘텀이 붙는다면) 더 큰 단계를 선택.
                - 경사 방향이 서로 상반되면 더 작은 단계를 선택
                - 골짜기에 공을 굴린다고 생각했을 때 골짜기 바닥에 가까워질수록 모멘텀이 붙는다는 것.
            - 에이다그레이드 (조정적 경사 알고리즘) Ada Grad
                - 학습률을 매개변수에 맞게 변화시킴. 자주 발생하는 특성들과 관련된 매개변수는 작게 갱신하고 드물게 발생하는 특성들과 관련된 매개변수는 크게 갱신함.
            - 알엠에스프랍 (RMS Prop)
                - 제곱 평균 제곱근 전파의 줄임말.
                - 학습률이 자동으로 변하고 각 매개변수의 학습률이 다르다.
            - 아담 (조정적 순간 예측)
                - 에이다그레이드와 알엠에스프랍 알고리즘을 결합해 최적화를 수행함. 인기가 가장 많은 경사하강 최적화 알고리즘임.
         
        - 에폭
            - 전체 훈련 데이터셋에 대해 신경망을 갱산하는 주기를 에폭이라 함.
            - 데이터 크기와 계산적 제한에 따라 수십, 수백, 수천 번의 에폭으로 신경망을 훈련함.
        - 배치 크기
            - 정방향/역방향 전달로 훈련하는 예제의 횟수.
            - 배치 크기 32는 모델 가중치 갱신 전에 훈련셋으로부터 32개 샘플 사용해 오차 경사를 예측한다는 뜻.
            - 배치 크기가 클수록 메모리 공간 많이 필요.
     
### 3.2. 인공신경망 모델 생성
- 케라스 및 머신러닝 패키지 설치
    - 사용자 친화적인 딥러닝 파이썬 라이브러리.
    - 텐서플로와 테아노 같은 복잡한 수치 계산 엔진을 감싸는 wrapper에 불과하다.
    - 그래서 먼저 텐서플로와 테아노를 설치해야 함.
```python
# 케라스의 패키지 가져오기
from Keras.models import Sequential
from Keras.layers import Dense
import numpy as np

# 데이터 불러오기
# 넘파이 모듈로 임의의 데이터와 분류 생성하기.
# (1000, 10) 크기 데이터 정렬을 먼저 만들고, 0과 1로 이루어진 (1000, 1)  크기의 분류 정렬을 만든다.
data = np.random.random((1000, 10))
Y = np.random.randint(2, size=(1000, 1))
model = Sequential()
```
- 모델 생성 - 신경망 구조
    - 선형적으로 층을 쌓는 케라스 시퀀셜 모델 사용.
    - 시퀀셜 모델 생성하고, 망 구조 완성될 때까지 한 번에 하나씩 층을 추가해 나감.
        - 입력층 입력 개수가 적당한지 가장 먼저 확인.
        - 첫 번째 층 생성할 때 입력 개수 지정.
        - 그 다음 인수 input_dim을 사용해 입력층을 다루고 있음을 나타내기 위해 밀집층dense layer 혹은 완전 연결층을 선택한다.
        - add() 함수로 모델에 층 하나 추가하고 각 층의 노드 수를 지정한다.
        - 출력층으로 또 다른 밀집층 하나 추가한다.
    - 구현하려는 모델 구조는 다음과 같음.
        - 모델에는 10개의 변수가 있다. input_dim=10 인수
        - 첫 은닉층에는 노드 32개, relu 활성화함수.
        - 두 번째 은닉층은 노드 32개, relu 활성화 함수
        - 출력층은 노드 1개, sigmoid 활성화 함수.
```python
model = Sequential()
model.add(Dense(32, input_dim=10, activation= 'relu'))
model.add(Dense(32, activation = 'relu' ))
model.add(Dense(1, activation = 'sigmoid'))
```
- 모델 컴파일
    - compile() 함수로 모델 컴파일 가능.
    - 모델 컴파일이란, 테아노나 텐서플로 패키지에 있는 수치 계산 라이브러리 이용하겠다는 것.
    - 컴파일에서는 신경망 훈련에 필요한 추가 속성을 지정하는 것이 중요.
    - 하나의 신경망 훈련이라는 것은, 문제에 대해 실제 예측하는 가중치의 최적 집합을 찾는 과정이다.
    - 따라서 가중치 집합 평가를 위한 손실함수, 신경망에 대한 다른 가중치 찾으려는 최적화 알고리즘, 훈련 중 필요한 매크릭 지정해야 함.
        - 다음 예제는 cross-entropy 손실 함수 사용. 케라스에서는 binary_crossentropy이다.
        - 아담 최적화 알고리즘 사용하고, 분류형 문제이기 때문에 매트릭으로 분류 정확도도 출력하는 코드이다.
```python
model.compile(loss= 'binary_corssentropy', optimizer = 'adam' , \
    metrics = ['accuracy'])
```
- 모델 적합화
    - fit() 함수를 호출하여 모델 훈련하거나 데이터에 적합화 할 수 있다.
    - 훈련 과정은 nb_epoch를 이용해 지정된 반복 횟수(에포크) 동안 실행됨.
    - 신경망에서 가중치가 갱신되기 전에 인수 batch_size를 이용해 배치 크기 정할 수 있음.
    - 이 변수들은 시행착오를 통해 실험적으로 정할 수 있다.
```python
model.fit(data, Y, nb_epoch=10, batch_size=32)
```
- 모델 평가
    - evaluation() 함수로 훈련셋에 대해 모델 평가 가능.
    - 이 함수는 각 입력, 출력 쌍에 대해 예측하고, 평균손실, 정확도 같이 설정된 매트릭을 포함해 점수를 기록한다.
```python
scores = model.evaluate(X_test, Y_test)
print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
```
- 더 빠른 인공 신경망 모델 실행: GPU와 클라우드 서비스
    - 거대한 계산 능력이 필요
        - CPU는 로컬 머신에서 명령어 처리와 실행을 책임짐.
        - CPU는 코어가 제한적이고 일을 순차적으로 처리하여 딥러닝 훈련에 필요한 행렬 계산을 신속하게 처리하지 못함.
        - 3가지 방법이 있다.
            - GPU가 있는 컴퓨터에서 실행
            - 캐글 커널 또는 구글 코랩에서 실행
            - AWS 이용
- 그래픽 처리 장치 GPU
    - 수천 개 스레드를 동시에 처리할 수 있는 수백 개의 코어로 구성 됨.
    - 특히 복잡한 행렬 연산 처리에 적합함.
- 클라우드 서비스
    - 캐글: 구글이 소유한 데이터 공학 웹사이트. 주피터 서비스를 제공하는데 캐글 커널이라고 함.
        - 무료 서비스이고 흔히 사용하는 패키지가 설치되어 있다.
        - 커널을 캐글 데이터 셋과 연결하거나, 진행 과정 중 새로운 데이터 셋 업로드도 가능함.
    - 구글 코랩: 주피터 노트북 환경에서 GPU 무료로 사용 가능.
    - 아마존 웹 서비스: ASW 딥러닝은 어떤 규모든 클라우드에서 딥러닝 가속화 가능한 하부구조 제공.
        - AWS 서버 인스턴스를 빠르게 실행 가능.
        - 인기 있는 딥러닝 프레임워크가 이미 설치되어 있고, 사용자 정의 인공지능 모델을 훈련하고 새로운 알고리즘 실험하며
        - 새로운 기술, 방법을 배우는데 필요한 인터페이스를 제공함.
        - AWS 서버는 캐글 커널보다 더 오래 실행 가능하다. 대규모 프로젝트라면 AWS를 추천.
- 맺음말
    - 인공신경망은 머신러닝 모든 유형에 사용되는 알고리즘의 집합체.
    - 동물 뉴런과 뉴런층이 있는 생물학적 신경망에 영감을 받음.
    - 다수의 층이 있다면 심층 신경망.
## 4장. 지도학습: 모델 및 개념
- 지도학습
    - 선택된 알고리즘이 주어진 입력값을 이용해 타겟을 적합화하는 머신러닝 영역이다.
    - 참값(레이블)이 포함된 훈련셋이 제공되고, 규칙을 학습해 나간다.
    - 새로운 관찰(입력)에 대한 참값을 예측하기 위해서.
    - 회귀 알고리즘과 분류 알고리즘으로 나뉜다.
        - 회귀 알고리즘은 무한의 해결값(연속적 집합)을 가진 문제의 결괏값을 추정.
        - 분류 알고리즘은 확률 기반으로, 출력은 분류값이지만, 해당 분류에 속할 가장 높은 확률을 찾음.
    - 지도학습 모델은 금융 분야 머신러닝에서 가장 많이 사용하는 대표 분야.
    - 알고리즘 거래에도 대부분 지도학습을 기반으로.
    - 효율적 훈련이 가능하고, 이상치 금융 데이터에 비교적 안정적이고, 금융 이론과 강한 연관성이 있기 때문.
    - 회귀
        - 학계와 산업계 연구자들은 자산가치 산정 모델 개발을 위해 회귀 기반 알고리즘을 연구해옴.
        - 기간대비 수익 예측하고, 자산 수익을 이끄는 중대한 요소 찾는데 알고리즘을 사용함.
        -  회귀 기반 지도학습은 포트폴리오 관리 및 파생 상품 가격 책정에 활용된 사례가 많음.
    - 분류
        - 분류 기반 알고리즘은 분류적 결과 도출하거나 예측하는데 사용.
        - 사기 감지, 채무 불이행 예측, 신용점수, 자산가치 동향 에측, 매수/매도 추천, 포트폴리오 관리, 가치 산정 등
### 지도학습 모델: 개념
- 분류와 회귀는 다르다.
    -  분류: 이산형 참값(레이블)을 예측
    - 회귀: 연속형 값을 예측.
        - 그러나 두 문제는 비슷하다.
        - 일부 모델은 수정 조금만 하면 분류, 회귀 모두에 사용 가능.
        - 선형 모델, 로지스틱 회귀는 둘 다 사용하지는 못한다.
- 지도학습
    - 회귀모델
        - 선형회귀, 정규화 회귀, K-최근접 이웃, 결정트리 회귀(CART), 서포트 벡터 회귀
        - 앙상블 부스팅: 에이다부스트, 경사부스팅 방법
        - 앙상블 배깅: 랜덤포레스트 방법, 엑스트라 트리
        - 인공 신경망: 인공 신경망(딥러닝 포함)
    - 분류모델
        - 로지스틱 회귀, 선형 판별 분석, K-최근접 이웃, 결정 트리 분류기, 서포트 벡터 분류기
        - 앙상블 부스팅: 에이다부스트, 경사부스팅 방법
        - 앙상블 배깅: 랜덤포레스트 방법, 엑스트라 트리
        - 인공 신경망: 인공 신경망(딥러닝 포함)
- 금융에서는 같은 시계열 동안 미래 값 예측을 위해 이전에 관찰된 데이터에서 신호를 추출하는 모델에 중점을 둔다.
- 시계열 모델은 연속 출력을 예측하고 지도 회귀 모델과 더 잘 정렬되며, 지도 회귀를 다루는 5장에서 별도로 다룰 것.
1. 선형 회귀(최소 제곱 회귀, OLS 회귀)
    - 입력변수 x와 단일 출력 변수 y 간 선형 관계가 있다고 가정함.
    - beta_0은 인터셉트(y절편)이고, beta_i 는 회귀계수이다.
    - 모델 훈련은 비용함수(손실함수) 최소화하는 방식으로 모델 매개변수를 도출한다.
    - 비용함수(손실함수) 정의하기
        - 비용함수는 모델 예측이 얼마나 부정확한지 측정.
        - RSS(잔차 제곱합)은 실젯값과 예측값 차의 제곱의 합이다.
    - 비용을 최소화하는 매개변수
        - 음의 수를 피하고 더 큰 차잇값에 가중 손실을 주기 위해서 제곱을 한다. 그 값들을 더하고 평균을 구한다.
        - 이 값이 데이터가 하나의 선에 얼마나 잘 적합하는지를 나타내는 척도가 된다.
    - 격자 탐색
        - 가능한 하이퍼파리미터 조합을 모두 생성하고, 그 조합들로 하나씩 모델 훈련하는 것.
        - 하이퍼파라미터는 모델의 외적 특성이고, 모델 세팅으로 생각할 수 있으나, 모델의 매개변수 기반으로 추정하지는 않는다.
        - 격자 탐색을 통해 하이퍼파라미터가 조절된다.
        - 격자 내에서 최적의 매개변수를 찾아내고 만다.
        - 단점은 매개변수가 추가되거나 고려되는 값이 클수록 격자 크기가 기하급수적으로 증가.
        - 사이킷런 패키지의 model_selection 모듈의 GridSearchCV 클래스로 검증 가능.
            - 첫 단계에서는 모델 객체를 생성.
                - 그 다음, 키워드가 하이퍼파리미터이고 값이 테스트할 매개변수 세팅을 나열하는 딕셔너리 정의.
                - 선형회귀는 fit_intercept이고, 부울 변수로 이 모델에 인터셉트 계산할지 여부를 결정.
                - False는 인터셉트 계산하지 않음.
            - 두 번째 단계는 GridSearchCV 객체를 생성하고, 초기화를 위해 추정 객체와 매개변수 격자, 점수화 방법, 교차 검증 선택을 제공함.
                - 교차 검증은 리샘플링 절차로 머신러닝 모델을 평가하는데 사용됨.
                - 점수화 매개변수는 모델의 검증 매트릭임.
```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, Y)

# 격자 탐색
model = LinearRegression()
param_grid = {'fit_intercept': [True, False]}

grid = GridSearchCV(estimator = model, param_grid = param_grid, scoring = 'r2', cv=kfold)
grid_result = grid.fit(X, Y)
```
- 장단점
    - 이해하고 해석하기 쉽다
    - 예측되는 변수와 예측 변수 관계가 비선형이면 올바르게 작동하지 않을 수도.
    - 과적합에 취약하고, 특성이 많은 경우 무관한 특성을 효과적으로 처리하지 못할수도.
    - 데이터가 어떠한 가정, 예를 들면 다중공선성이 없다는 가정을 따라야 함.
2. 정규화 회귀
- 선형 회귀에 독립변수가 많으면, 계수가 부실하게 결정되고 모델이 과적합될 수도. 이를 과적합 또는 고분산이라고 함.
- 과적합 조절의 일반적인 방법은 정규화.
    - 손실함수에 패널티 항을 추가해 계수가 큰 값에 이르는 것을 억제하는 것.
    - 정규화는 예측 정확도와 해석이 더 우수한 모델 만들기 위해 모델 매개변수 영향을 (0에 가깝게) 축소하는 패널티 매커니즘임.
    - 2가지 이점.
        - 예측 정확도
            - 테스트 셋에 더 잘 일반화 된다는 것. 매개변수가 너무 많은 모델은 노이즈에 적합화 되는 경향.
            - 일부 계수를 0에 근접시켜서 복잡한 모델(과도한 치우침)을 줄이는 저분산 모델로 적합화 가능.
        - 해석
            - 예측 변수가 많으면 결과 큰 그림 해석하거나 전달하기 복잡할수도.
    - 정규화 방법
        - L1 정규화 (라쏘 회귀)
            - 선형 회귀 비용 함수 RSS에 있는 계수의 절댓값을 더한다.
            - 계수가 0의 값으로 진행된다. 즉, 일부 특성이 출력 평가에서 완전히 무시됨.
            - 정규화 매개변수 lambda 값이 더 클수록 더 많은 특성이 0의 값으로 수렴함.
            - 일부 특성 완전히 제거하고 예측 계수의 하위셋을 갖게 되며, 결과적으로 모델 복잡도 감소.
            - 따라서 과적합을 줄이고 특성 선택에도 유용한 방법.
            - 예측 계수가 0의 값으로 수렴하지 않는다는 것은 그 예측 계수가 중요하다는 의미이기 때문.
                - 이를 바탕으로 특성 선택(희소 선택)이 가능.
                - lambda의 값을 0으로 하여 기본선형회귀식을 얻을 수도.
        - L2 정규화 (릿지 회귀)
            - RSS에 있는 계수의 제곱을 더한다.
            - 계수에 제약을 둔다.
            - 패널티 항(lambda)는 계수를 정규화하는데, 계수가 큰 값을 취하면 최적함수에 패널티를 줌.
            - 계수를 작게 하면 더 작은 분산과 오차값을 유도함.
            - 릿지 회귀는 모델 복잡도는 줄이지만 변수 개수는 줄이지 않는다. 다만 변수 영향을 줄일 뿐.
            - 람다가 0에 근접할 때 비용함수는 선형회귀 비용함수에 가까워짐.
            - 따라서 특성에 대한 제약이 낮아지면 (낮은 람다), 모델은 선형회귀모델에 더 가까워짐.
        - 엘라스틱 넷
            - 정규화 항을 모델에 더하는데, L1, L2 정규화를 합한 것이 된다.
            - lambda 값을 정할 뿐 아니라, 알파 매개변수를 조절할 수 있다.
            - alpha가 0이면 릿지, 1이면 라쏘에 해당한다.
            - alpha를 0~1 사이로 조절하면서 엘라스틱 넷을 최적화 함.
            - 일부 계수를 줄이고 희소 선택에 대한 일부 계수를 0으로 한다.
    - 모든 정규화 회귀에서 lambda는 핵심 매개변수로 파이썬에서 격자 탐색하는 동안 조절됨.
    - 엘라스틱 넷에서는 alpha가 추가 매개변수로 조절됨.
```python
# L1 라쏘
from sklearn.linear_model import Lasso
model = Lasso()
model.fit(X, Y)

# L2 릿지
from sklearn.linear_model import Ridge
model = Ridge()
model.fit(X, Y)

# 엘라스틱 넷
from sklearn.linear_model import ElasticNet
model = ElasticNet()
model.fit(X, Y)
```
3. 로지스틱 회귀 logistic regression
- 분류 문제에 가장 널리 사용되는 알고리즘.
- 출력 클래스의 확률 분포를 모델링 함.
- x에 대한 선형 함수가 가장 많이 사용 됨. 출력 확률은 0~1이고 합은 1.
- sigmoid 함수를 적용 해 0~1 사이의 확률을 출력하도록 선형 회귀를 변형할 수 있다.
    - 입력 데이터에 각 행에는 beta 계수가 포함됨.
    - 이 계수는 훈련데이터로 학습한다.
- 비용함수는 실젯값이 0일 때 얼마나 자주 1로 예측했는지 (그 역도 포함) 측정값으로 나타냄.
- 최대우도측정 MLE 같은 기술을 사용해 기본 클래스는 1에 가까운 값, 다른 클래스는 0에 가까운 값을 에측한다.
- 하이퍼파라미터
    -  정규화 (사이킷런의 penalty)
        -  L1, L2, 엘라스틱넷 정규화 적용 가능. 사이킷런 라이브러리 내 값은 l1, l2, elasticnet이다.
    -  정규화 강도(사이킷런의 C)
        -  패널티 매개변수의 적정 값은 100, 10, 1, 0.1, 0.01 이다.
-  장단점
    -  구현하기 쉽고, 해석성 좋고, 선형적으로 분리된 클래스에서 잘 작동.
    -  모델 출력은 확률로 나타내고, 순위 정하는데도 유용.
    -  모델은 하이퍼파리미터 개수가 적다.
    -  과적합 위험 가능성있지만, 정규화를 통해 문제 해결 가능.
    -  반면, 특성의 수가 커지면 과적합되는 가능성.
    -  선형함수만 학습 가능하고 특성과 목표 변수의 관계가 복잡할 경우 덜 적합함.
```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X, Y)
```
4. 서포트 벡터 머신 SVM
- 마진을 최대화 하는 것.
    - 마진은 분리 초평면(혹은 결정선)과 이 초평면 가장 가까이에 있는 샘플 사이의 거리로 정의됨.
    - 마진은 마진 영역 중앙선에서 가장 가까운 포인트까지의 수직거리로 계산.
    - 모든 데이터 포인트에 대해 균일하게 구분짓는 최대 마진 영역을 계산하는 것.
    - 실제 데이터는 깔끔하게 초평면으로 완벽하게 구분할 수는 없다.
        - 그래서 클래스 구분 중앙선 마진의 최대화하는 조건을 완화해야 함.
        - 그러면 일부 데이터 포인트가 중앙선을 침범하게 된다.
        - 각 차원에 꾸불꾸불한 마진 영역을 허용하는 추가적인 계수집합이 도입되고, 모든 차원에 꾸불꾸불한 정도를 나타내는 C라는 튜닝 매개변수가 도입됨.
        - C가 클수록 초평면이 더 많이 침범됨.
    - 일부 경우 초평면 혹은 선형결정영역을 찾지 못할 수도. 이 때는 커널을 사용.
        - 커널은 SVM 알고리즘이 많은 데이터를 쉽게 처리할 수 있도록 입력 데이터를 변환하는 것.
        - 커널 사용해 원래 데이터를 고차원에 투영하여 데이터를 더 잘 분류함.
    - SVM은 회귀, 분류 모두 사용됨.
        - 본래의 최적화 문제를 두 개의 문제로 나눔으로써 가능한 것이다.
        - 회귀문제에 대해서는 목표가 반대이다.
            - 마진 침범 제한하면서 두 클래스 사이의 마진 영역을 최대한 넓히는 대신, 가능한 많은 점을 마진 영역에 맞춰지게 한다.
            - 그리고 마진 영역 넓이는 하이퍼파라미터로 튜닝한다.
    - 하이퍼파리미터
        - 커널 (사이킷런의 kernel)
            - 커널 선택은 입력변수 투영 방식을 결정. 선형 커널과 RBF가 가장 보편적.
        - 패널티 (사이킷런의 C)
            - SVM 최적화에서 훈련 예제의 오분류를 얼만큼 회피할지 정도를 나타냄.
            - 이 값이 크면 최적화는 마진이 작은 초평면을 선택함. 로그 스케일로 10~1000 사이 값이 적당하다.
- 장단점
    - 과적합에 안정적. 고차원 영역일수록 좋다.
    - 비선형 관계를 다루는 커널이 다양하고 데이터 분포도를 요하지 않는 장점.
    - 반면, 학습에 비효율적이며 메모리 많이 요구.
        - 큰 데이터셋에서 좋지 않다.
    - 데이터의 스케일링 필요.
    - 하이퍼파라미터가 많으나 그 의미가 직관적이지 않다는 단점.
```python
# 회귀
from sklearn.svm import SVR
model = SVR()
model.fit(X, Y)

# 분류
from sklearn.svm import SVC
model = SVC()
model.fit(X, Y)
```
5. K-최근접 이웃 KNN
- 모델에서 요구하는 학습이 없어서 '게으른 학습자'라는 별칭이 붙음.
- 새로운 데이터에 대해, 전체 훈련셋을 통해 그 데이터가 가장 근접한 K개의 이웃을 찾고 그에 대핸 출력 변수를 도출하는 방식으로 예측한다.
- 가장 유사한 K개 이웃을 결정하기 위해 거리를 측정한다.
    - 가장 보편적인 거리 측정 법은 유클리드 거리 Euclidean distance.
        - 입력 변수가 형태적으로 비슷한지 측정하는데 유용.
    - 또 다른 매트릭으로는 맨해튼 거리 Manhattan distance.
        - 입력변수가 형태적으로 다른지 측정하는데 유용.
- 하이퍼파라미터
    - 이웃 수(사이킷런의 n_neighbors)
        - 이웃 수이며, 1~20 이 적당.
    - 거리 메트릭 (사이킷런의 metric)
        - 여러 거리 매트릭 사용하는 것도 흥미로움. 유클리언과 맨해튼이 적당.
- 장단점
    - 학습이 필요없고 학습 단계가 없다.
    - 알고리즘이 예측 수행 전에 학습하지 않으며, 알고리즘 정확도에 영향 주지 않고 새로운 데이터 추가 가능.
    - 직관적으로 쉽게 이해 가능.
    - 모델은 다중 분류를 다루고 복잡한 결정선도 학습 가능.
    - 큰 학습 데이터에 효과적. 잡음 있는 데이터에 안정적이라 이상치를 걸러낼 필요가 없음.
    - 선택할 거리 매트릭이 불명확하고 많은 경우 거리 매트릭을 정당화하기 어려운 단점.
    - KNN은 고차원 데이터셋에서 좋은 성능을 보이지 않음.
    - 모든 이웃 거리를 계산해야 해서 비용이 높아지고 새로운 예를 예측하는 성능이 느림.
    - 데이터셋에 있는 잡음에 민감하여 결측값을 수동으로 입력하고 이상치를 제거해야 함. 이 경우, 잘못된 예측의 가능성.
```python
# 분류
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier()
model.fit(X, Y)

# 회귀
from sklearn.neighbors import KNeighborsRegressor
model = KNeighborsRegressor()
model.fit(X, Y)
```
6. 선형 판별 분석 LDA(linear discriminant analysis
- 분류 분별력을 극대화하고 분류 내의 분산을 최소화하는 방식으로 데이터를 저차원 영역에 투영함.
- 모델을 훈련하는 동안 각 분류에 대한 통계적 특성인 평균과 공분산 행렬을 계산한다.
- 통계적 특성은 다음 가정을 기반으로 평가한다.
    - 데이터는 정규분포를 따른다.
    - 각 속성 분산이 같고 각 변수 값은 평균에서 동일한 양으로 변한다.
- LDA는 예측을 하기 위해 새로운 입력 집합이 각 클래스에 속할 확률을 계산한다. 출력 클래스는 확률이 가장 높은 클래스이다.
- 파라미터
    - 차원 축소를 위한 컴포넌트의 수 (number of components)로 사이킷런의 n_components
- 장단점
    - 비교적 단순한 모델로 빠르고 쉽게 구현하는 장점.
    - 특성 스케일링이 필요하고 복잡한 행렬 연산을 요구하는 단점.
```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
model = LinearDiscriminantAnalysis()
model.fit(X, Y)
```
7. 분류 트리와 회귀 트리
- 트리
    - 케이스의 정확한 분류/예측을 위한 논리적 if-then 조건의 집합을 만든다.
    - 데이터를 쪼개고 일련의 질문에 대한 결과로 결정해 나간다고 볼 수 있다.
    - 랜덤포레스트와 경사 부스팅 같은 앙상블 기법의 토대가 됨.
    - 모델의 표현
        - 이진 트리(결정 트리)로 나타낸다.
- CART 모델 학습
    - 이진트리 생성은 실제로 입력 영역을 나누는 과정.
    - 재귀적 이분법이라는 탐욕적 접근법을 이용해 영역을 나눈다.
        - 모든 값을 정렬하고 비용 함수를 이용해 분기점을 테스트하는 수치적 절차이다.
        - 가장 낮은 비용의 분기를 선택한다.
        - 모든 입력변수와 모든 가능한 분기점을 탐욕적 방식(매번 최선의 분기를 선택)으로 평가하고 선택함.
    - 분기점 선택을 위한 최소비용 계산하는 비용함수는 사각형 영역 내에 속하는 모든 훈련 샘플에 대한 오차 제곱합으로 나타냄.
        - 분류 모델에서는 지니비용함수를 사용. 말단 노드가 얼마나 단일한지(얼마나 섞여있는지)를 표시한다.
        - 완전 단일 노드는 0이 되고, 50대50으로 나뉘어지면 0.5가 된다.
    - 기준 정지
        - 언제 분기를 멈출지 알아야 한다.
        - 말단 노드 훈련셋의 개수를 세어서 결정.
    - 가지치기
        - 기준 정지하기는 트리 성능에 직접 영향을 준다는 점에서 중요.
        - 성능 향상을 위해 트리 학습 후 가지치기를 한다.
        - 결정 트리 복잡도는 분기 수로 정의한다.
        - 단순해야 과적합을 줄일 수 있다.
        - 말단을 하나씩 제거해가며 결과를 평가하는 방법이 가장 쉽다.
        - 전체 비용 함수값이 떨어지는 경우 말단 노드를 제거하는 것. 더 이상 제거할 수 없을 때까지.
- 하이퍼파라미터
    -  많은 하이퍼파라미터가 있지만, 핵심은 트리 모델 최대 깊이.
        -  차원 축소에 필요한 구성요소의 수에 해당하고, max_depth로 나타냄. 2~30이 적당한 범위.
-  장단점
    -  해석하기 쉽고 복잡한 관계성 학습에 용이.
    -  적은 데이터를 요구하고 스케일링도 필요없음.
    -  특성 중요도는 결정 노드 구축 방법에 따라 결정됨.
    -  대용량 데이터셋에서 좋은 성능 보이고, 회귀/분류 모두 사용 가능.
    -  CART는 가지치기를 사용하지 않으면 과적합 되기 쉬움.
    -  훈련데이터셋을 약간만 변화시켜도 가정 함숫값의 차이가 급격해질 수 있다.
    -  일반적으로 앙상블 모델보다는 나쁜 성능을 보임.
```python
# 분류
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier()
model.fit(X, Y)

# 회귀
from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor()
model.fit(X, Y)
```
8. 앙상블 모델 ensemble model
- 앙상블
    - 여러 분류기를 합쳐 더 좋은 메타 분류기를 만드는 것.
    - 배깅과 부스팅이 있다.
        - 배깅(부스트랩 집합)은 병렬적으로 여러 모델 훈련하는 것.
            - 각 모델은 데이터의 임의 하위셋으로 훈련한다.
        - 부스팅은 순차적으로 여러 모델을 훈련.
            - 하나의 모델 먼저 생성한 후, 첫 모델의 오류를 보정하는 방식으로.
            - 훈련셋을 이용한 예측이 완전히 종료되거나
            - 최대 허용 모델 생성 수에 도달할 때까지 모델을 계속 생성한다.
            - 각 모델은 이전 실수를 학습한다.
        - 개별 모델을 합치면서 더 유연해지고(편향이 적어지고), 데이터에 덜 민감해진다(분산이 적어진다).
        - 단순한 알고리즘을 합쳐 성능을 향상시키는 것.
- 랜덤 포레스트
    - 배깅 결정트리가 변형된 것. 1000개의 예가 있는 데이터셋을 예시로 배깅 진행해본다.
        - 1) 데이터셋에서 임의의 많은(가령 100개) 샘플셋 선택.
        - 2) 각 샘플 데이터셋으로 CART를 훈련함.
        - 3) 새로운 데이터셋이 주어지면, 각 모델 평균 예측을 계산하고, 각 트리에서 얻은 예측을 모아 과반수 투표로 최종 결과 결정.
    - CART 같은 결정트리 문제는 탐욕적 접근법을 사용한다는 것.
        - 오류를 최소화하는 탐욕적 알고리즘을 사용해 분기할 변수를 선택함.
        -  배깅이 끝난 후에도 결정트리는 구조적으로 유사할 수 있으며, 이로써 각 트리 예측이 서로 높은 상관관계를 가짐.
        -  이렇게 모델 예측을 통합할 수 있고, 만약 모델 예측 간 상관관계가 없거나 낮으면 통합 예측의 성능이 훨씬 더 좋아진다.
        -  즉, 랜덤포레스트는 하위 트리 예측의 상관관계가 낮아지는 방식으로 학습 알고리즘을 변경하는 것.
    -  CART에서 분기점 선택할 때 학습알고리즘은 모든 변수와 그 값을 훑어볼 수 있는데, 최적의 분기점을 찾기 위해서이다.
        -  그러나 랜덤포레스트는 분기점 선택할 때 각 하위트리가 특성의 임의의 샘플에만 접근할 수 는 방식으로 진행한다.
        -  각 분기점 (m)에서 찾을 특성의 수는 알고리즘의 매개변수로 정해야 함.
    -  배깅 결정 트리가 만들어지면, 비용함수가 변수에 대해 각 분기점에서 얼마나 많이 떨어지는지 계산 가능.
    - 회귀면 오차제곱합으로, 분류면 지니비용으로 표현 가능.
    - 배깅 방법은 개별 변수에 대해 비용함수 계산하고 평균 계산하여 특성 중요도를 나타냄.
- 하이퍼파라미터
    - 최대 특성 수 max_features
        - 가장 중요. 각 분기점에서 샘플링할 임의 특성의 수.
        - 입력 특성 수를 절반으로 나누거나 1~20 사이 같은 정수 구간을 선택함.
    - 평가자 수 n_estimators
        - 트리 수를 나타냄.
        - 원칙적으로 모델이 더 이상 향상되지 않을 때까지 증가시킨다. 10~1000까지의 로그 스케일이 적당.
- 장단점
    - 우수한 성능, 확장성, 용이한 사용성 덕분에 널리 사용.
    - 유연하며, 특성 중요도를 점수로 나타내고, 중복되는 특성 열을 처리 가능.
    - 큰 데이터 셋에 적용 가능하고 과적합에 안정적.
    - 데이터 스케일링 필요 없고 비선형 관계도 모델링 가능.
    - 단, 블랙박스 접근법과 같아서 모델 동작 제어가 거의 불가, 결과 해석이 어려움.
    - 분류에서 우수한 성능을 보이지만, 정확한 연속성 예측을 못해서 회귀에 적합하지 않음.
        - 회귀일 때 훈련셋을 벗어나 예측하지 못하고, 잡음 섞인 데이터셋에 과적합될 가능성이 있다.
```python
# 분류
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(X, Y)

# 회귀
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X, Y)
```
- 엑스트라 트리 (완전 임의 트리)
    - 랜덤포레스트가 변형된 것.
    - 임의 특성 하위셋으로 다중 트리를 생성하고 노드를 분할함.
    - 대체하여 관찰하는 랜덤포레스트와 달리, 엑스트라 트리는 대체하지 않고 관찰함. 따라서 관찰을 반복하지는 않음.
    - 랜덤 포레스트는 부모 노드를 두 개의 동종 자식 노드로 변환하기 위한 최선의 분기를 선택하지만,
    - 엑스트라트리는 부모노드를 두 개의 임의 자식 노드로 변환하기 위해 임의 분기를 선택한다.
        - 임의성은 데이터를 임의 선택하지 않고 관찰을 임의 분기하는 것에서 비롯된다.
    - 성능과 장단점은 랜덤포레스트와 비슷하다.
```python
# 분류
from sklearn.ensemble import ExtraTreesClassifier
model = ExtraTreesClassifier()
model.fit(X, Y)

# 회귀
from sklearn.ensemble import ExtraTreesRegressor
model = ExtraTreesRegressor()
model.fit(X, Y)
```
- 어답티브 부스팅(에이다 부스팅)
 - 순차적 예측으로 후속 모델이 앞선 모델 오류를 최소화하는 방식으로 진행하는 아이디어에 바탕.
 - 매번 각 예시의 가중치를 변경하면서 분포를 바꾼다.
 - 잘못 예측한 예의 가중치를 높이고 옳게 예측한 예의 가중치는 낮춘다.
 - 다음의 단계로.
  - 1) 처음에는 모든 관찰에 대한 가중치 동일하게
  - 2) 일부 데이터로 모델 생성하여 전체 데이터셋에 대해 예측. 예측과 실제 오차 계산.
  - 3) 잘못 예측한 데이터의 가중치를 높임. 가중치는 오차 크기에 따라 결정.
  - 4) 비용함수가 더 이상 변하지 않을 때까지, 혹은 예측 횟수가 최대에 도달할 때까지 반복
- 하이퍼파라미터
    - 학습률 learning_rate
        - 각 분류기의 기여도를 로그 스케일로 축소함. 0.001, 0.01, 0.1이 적당.
    - 예측 횟수 n_estimators
        - 트리의 수를 나타냄. 더 이상 향상이 없을 때까지 증가시키는 것이 원칙. 로그 스케일로 10~1000이 적당.
- 장단점
    - 고도의 정확성. 다른 모델에 비해 매개변수 튜닝 적게 하면서 비슷한 결과를 만듦.
    - 데이터 스케일링 필요 없고, 비선형 관계 모델링도 가능.
    - 훈련에 많은 시간 소요됨.
    - 잡음과 이상치에 민감, 불균형 데이터는 분류 정확도를 낮춤.
```python
# 분류
from sklearn.ensemble import AdaBoostClassifier
model = AdaBoostClassifier()
model.fit(X, Y)

# 회귀
from sklearn.ensemble import AdaBoostRegressor
model = AdaBoostRegressor()
model.fit(X, Y)
```
- 경사 부스팅 방법
    - 순차적으로 예측하는 측면에서 에이다 부스트와 유사. 이전에 적합화되지 않은 예측을 앙상블에 추가하고 이전 오류가 맞는지 확인.
        - 1) 일부 데이터로(최초의 취약한 학습자라고 할 수 있는) 모델 구축. 이 모델로 전체 데이터셋에 대해 예측
        - 2) 오차함수로 오차 계산
        - 3) 이전 오차를 목표 변수로 사용해 새로운 모델 생성. 목적은 오차 최소화하는 최적의 데이터 분기 찾기.
            - 새로운 모델로 얻은 예측을 이전 예측에 통합. 이것으로 얻은 오차를 계산.
        - 4) 비용함수의 개선이 없거나, 예측횟수가 최대치가 될 때까지 반복.
    - 매번 예의 가중치를 튜닝하는 에이더부스트와 반대로, 경사 부스팅은 이전 예측 오차를 보정하는 방식으로.
    - 장단점
        - 랜덤포레스트와 같은 방법으로 데이터 오류, 높은 상관관계 특성, 무관한 특성에 안정적.
        - RF보다 더 좋은 성능 보이면서 특성 중요도를 점수로 나타냄.
        - 부스팅 방법 주 목적이 분산이 아닌 편향 줄이는 것인만큼 RF보다 과적합에 더 취약.
        - 튜닝할 하이퍼파라미터 수가 많아서 모델 개발이 느리다.
        - 특성 중요도가 훈련셋 변화에 안정적이지 않음.
```python
from sklearn.ensemble import GradientBoostingClassifier
model = GradientBoostingClassifier()
model.fit(X, Y)
from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor
model.fit(X, Y)
```
9. 인공신경망 모델
- 인공신경망
    - 신경망은 출력층 노드 활성화 함수로 분류모델, 회귀모델에 맞게 축소 가능.
        - 회귀 문제는 출력 노드에 선형 활성화 함수가 있거나 함수가 없음.
            - 선형함수는 -inf~+inf에 이리는 출력 범위 내에서 연속성 출력을 생성ㅇ함.
            - 따라서 출력층은 출력층 이전 층에 있는 노드를 갖는 선형함수가 되며, 선형모델이다.
        - 분류 문제는 출력노드에 시그모이드 혹은 소프트맥스 활성화 함수.
            - 0~1 사이의 출력을 생성하는데, 이는 기댓값의 확률.
            - 소프트맥스 함수는 분류에서 다중 그룹에 사용됨.
    - 하이퍼파라미터
        - 은닉층 hidden_layer_sizes
            - 층과 노드 수. i번째 요소는 i번째 은닉층의 뉴런 수를 나타냄.
        - 활성화 함수 activation
            - 은닉층의 활성화 함수. sigmoid, relu, tanh 등 사용 가능.
    - 심층 신경망
        - 은닉층이 하나 이상 있는 인공 신경망.
        - 심층 신경망 구현을 위해서 케라스 라이브러리 사용.
        - 사이킷런의 MLPClassifier, MLPRegressor와 비슷하게 케라스에는 KerasClassifier, KerasRegressor 모듈이 있음.
        - 금융 분야에서는 주로 시계열 예측을 하는데, 과거 내용 기반으로 예측하는 것.
            - 시계열 예측 문제에 순환신경망RNN 같은 일부 심층 신경망 바로 사용 가능.
    - 장단점
        - 변수 간 비선형 관계를 잘 나타내는 것. 쉽게 학습하고 큰 데이터셋에서 나오는 막대한 양의 입력 특성을 잘 다룸.
        - 단점은 모델의 해석성. 이는 모델 결정에 매우 중요한 요소.
        - 문제 해결에 적합한 구조/알고리즘 선택이 어렵다.
        - 많은 연산이 필요해 훈련 시간이 길다.
    - 금융에서 지도학습에 인공신경망 사용
        - 선형, 로지스틱 같은 단순 모델 적합화하려면 굳이 인공신경망 사용할 필요는 없음.
        - 복잡한 데이터셋 모델링하고 더 강한 예측력을 위해서는 필요.
        - 인공신경망은 데이터 형태에 맞춰 적용하는 점에서 유연하고, 지도학습문제에 사용할만 하다.
```python
from sklearn.neural_network import MLPClassifier
model = MLPClassifier()
model.fit(X, Y)
from sklearn.neural_network import MLPRegressor
model = MLPRegressor()
model.fit(X, Y)
```
### 모델 성능 (p.95)
- 모델 성능 평가에 필요한 핵심 요소로 과적합, 교차검증, 평가메트릭을 살펴본다.
1. 과적합과 과소적합
- 머신러닝의 가장 보편적 문제는 과적합.
    - 훈련셋은 설명하지만, 테스트셋은 잘 설명 못하는 일반화의 부재.
    - 훈련셋에 과도하게 학습할 때 발생. 모델이 복잡해질수록 심각해진다.
- 과적합 과소적합 개념은 편향-분산 상충관계로 설명 가능.
    - 편향은 학습 알고리즘에서 지나치게 단순한 가정이나 잘못된 가정에서 나오는 오류.
        - 편향 결과로 과소적합 발생.
        - 편향이 큰 것은 알고리즘이 특성의 중요한 흐름을 놓치고 있는 것.
    - 분산은 모델이 훈련셋에 거의 완벽하게 적합할 정도로 복잡할 때.
        - 분산이 크면 모델 예측값이 훈련셋과 거의 일치.
        - 과적합 발생.
    - 좋은 모델은 편향과 분산이 모두 낮음.
- 과적합 해결법
    - 훈련 데이터 늘리기
    - 규제하기
        - 모델이 하나의 특성에 지나치게 많은 표현력을 보이거나 특성이 과도하게 많으면 비용 함수에 패널티 부과 가능.
- 과적합 개념과 감소법은 모든 지도학습 모델에 적용 가능.
    - 정규화된 회귀모델은 선형회귀의 과적합을 해결한다.
2. 교차 검증
- 머신러닝의 과제는 보이지 않는 데이터에도 잘 일반화되는 것.
    - 데이터를 한 번 이상 분할해서 분할한 데이터는 검증셋으로, 나머지는 훈련셋으로 사용하는 것.
    - K-겹 교차검증. 훈련데이터를 임의로 k등분함.k-1개로 훈련하고 1개로 검증하는 것. 그것을 k 번 반복하여 점수를 평균냄.
    - 잠재적 결점은 연산비용이다. 특히, 격자탐색과 병행하면 훨씬 증가함.
    - 사이킷런 패키지로 단 몇 줄로 교차검증까지 가능하다.
3. 평가 메트릭
- 메트릭 선택은 성능 측정하고 비교하는 방법에 영향을 줌.
    - 중요도 가중치 정하고, 궁극적인 알고리즘 선택에도 영향을 줌.
    - 회귀 문제: 평균절대오차MAE, 평균제곱오차MSE, 제곱오차R-square, 조정제곱오차Adj-R-squeare
    - 분류 문제: 정확성, 정밀성, 재현율, 곡선아래영역AUC, 혼동행렬
        - MAE: 오차의 크기는 알려주지만, 방향은 알려주지 않는다. 모두 동일한 가중치 부여.
        - MSE: 예측과 실젯값 차이에 대한 표준편차. 제곱근을 씌우면 출력과 같은 단위라 설명에 유리해짐. RMSE라 함.
        - 제곱오차: 예측값이 실젯값에 얼마나 적합한지. 0은 부적합 1은 완전적합.
        - 조정제곱오차: 모델에서 항의 수를 조정해 항이 곡선 및 직선에 얼마나 적합하는지. 조정제곱오차는 항상 제곱오차보다는 작다.
    - 예측정확도가 목표라면 RMSE가 가장 적합. 계산 단순, 구분이 쉬움. 손실이 대칭적이지만, 오차가 크면 더 많은 영향을 끼침.
    - 제곱오차와 조정제곱오차는 설명 목적으로 사용하며 독립변수가 종속변수 변화를 얼마나 잘 설명하는지를 나타냄.
        - 진양성, 위양성, 진음성, 위음성
        - 정밀성 = 진양성 / (진양성 + 위양성)
        - 재현율 = 진양성 / (진양성 + 위음성)
        - 정확성 = (진양성 + 진음성) / 전체
            - 정확성: 모든 예측 대비 정확히 맞힌 예측률. 가장 보편적으로 사용하지만, 잘못 사용하는 경우가 많음.
                - 각 분류에 속한 관찰의 수가 비슷한 경우(거의 드뭄), 해당 예측 오류가 동일하게 중요한 경우가 적합한데 이런 경우들은 드뭄.
            - 정밀성: 양성으로 예측한 것들 중에 실제 양성인 경우. 위양성에 대한 비용이 높을 때(ex 스팸 메일 감지)에서 적합.
            - 재현율(만감성, 진양성률): 실제 양성의 모든 경우 중 실제 양성인 경우. 분모는 양성으로 판정한 경우.
                - 위음성의 비용이 클 때(ex 사기 탐지)에서 적합함.
- ROC 곡선 아래 영역 area under ROC curve(AUC)
- 혼동행렬
### 모델 선택 (p.102)
- 모든 문제에 적합한 하나의 솔루션, 접근법은 없음. 가장 중요한 것은 성능이다. 그 외에도 다양.
1. 모델 선택 시 고려할 요소
- 단순성: 모델의 단순한 정도. 단순하면 빠르고 확장적이며 이해가 쉬움.
- 훈련 시간
- 데이터의 비선형성 처리: 모델이 변수 간 비선형적 관계를 처리하는 능력
- 과적합에 대한 안정성
- 데이터셋의 크기
- 특성 수
- 모델 해석: 얼마나 잘 설명해야 하느냐가 중요하다.
- 특성 스케일링: 스케일링할지 정규분포를 따를지.
- 모델 별 장단점
    - 선형회귀: 단순성, 훈련시간, 모델해석
    - 로지스틱: 단순성, 훈련시간, 모델해석
    - SVM: 단순성, 비선형처리, 과적합안정성, 특성수, 스케일링
    - DT: 단순성, 훈련시간, 비선형처리, 데이터셋크기, 특성수, 스케일링
    - 경사부스팅: 비선형처리, 데이터셋크기, 특성수, 모델해석
    - 랜덤포레스트: 비선형처리, 과적합안정성, 데이터셋크기, 특성수, 모델해석
    - 인공신경망: 비선형처리, 데이터셋크기, 특성수
    - KNN: 단순성, 훈련시간, 비선형처리, 과적합안정성, 모델해석
    - 선형판별분석: 단순성, 훈련시간, 비선형처리, 데이터셋크기, 특성수, 모델해석

2. 모델 균형
- 여러 요소를 절충해야 함.














