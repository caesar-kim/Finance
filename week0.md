# 0주차
# 1부. 프레임워크
## 1장. 금융 머신러닝
 - 금융에서의 머신러닝 활용
   - 1 알고리즘 트레이딩
   - 2 포트폴리오 관리 및 로보어드바이저
   - 3 이상 거래 탐지
   - 4 대출/신용카드/보험 계약 심사 (계약심사: underwriting)
   - 5 자동화와 챗봇
   - 6 위험 관리
   - 7 자산 가치 예측
   - 8 파생 상품 가격 책정
   - 9 감정 분석
   - 10 거래 결제
   - 11 돈세탁 방지
- 머신러닝의 유형
  - 지도학습(과제 기반), 비지도학습(데이터 기반), 강화학습(실수로부터의 학습)
    - 지도학습: 회귀호, 분류
    - 비지도학습: 차원축소, 군집화
    - 강화학습: 강화학습에는 분명한 답은 없음. 학습 시스템(에이전트)가 무엇을 할지 결정. 알고리즘이 경험을 통해 답을 결정한다.
    - 자연어 처리
## 2장. 머신러닝 모델 개발

  - 머신러닝에서 파이썬이 가장 지배적인 프로그래밍 언어.
  - 머신러닝에서 주로 사용하는 파이썬 패키지
    - 데이터 표현: NumPy, Pandas
    - 머신러닝, 통계분석: SciPy, Scikit-learn, StatsModels, Keras, Theano, TensorFlow
    - 데이터 시각화: Matplotlib, Seaborn
    - 패키지 관리: pip, Conda
      - 이 중 사이킷런과 케라스가 핵심 패키지. 
- 모델 개발 단계
  - 1 문제 정의 -> 2 데이터&패키지 불러오기 -> 3 탐색적 데이터 분석 ->
  - 4 데이터 준비 -> 5 모델 평가 -> 6 모델 튜닝 및 개선 -> 7 모델 결정
## 3장. 인공신경망
- 인공신경망ANN artificial neural networks
  - 딥러닝은 복잡한 인공신경망 알고리즘을 다룬다.
  - 복잡도는 모델 전체에서 정보 흐름을 나타내는 정교한 패턴에 좌우됨.
  - 딥러닝은 모종의 세계를 개념이 층층이 쌓인 계층적 구조로 표현 가능.
 - 인공신경망 구조: 뉴런, 계층, 가중치
 - 인공신경망 훈련: 순전파, 역전파, 경사하강법
 - 인공신경망 파라미터: 계층, 노드 수, 활성화함수, 손실함수, 학습률
### 3.1. 구조, 훈련, 하이퍼파라미터
  - 뉴런은 층을 이루며 복잡하게 배열됨.
  - 모델을 거쳐 얻은 결과와 에상치를 비교하는 방식으로 훈련 단계를 진행.
  - 이를 통해 데이터 내의 패턴을 인식하는 방법을 학습한다.
   - 뉴런(=인공 뉴런, 노드, 퍼셉트론)
    - 하나 이상의 입력과 출력은 단 하나.
     - 뉴런에 있는 활성화 함수가 여러 입력과 하나의 출력 간 복잡한 비선형적 함수의 매핑을 결정한다.
     - 입력(x_1, ..., x_n)을 받아서 훈련 매개변수 적용해 가중치 합(z) 생성한 후 활성화함수(f)에 전달하여 출력 계산f(z).
     - 하나의 뉴런에서 나온 출력만으로는 복잡한 계산 불가능.
     - 수직, 수평적으로 뉴런을 쌓아간다.
     - 입력층으로 입력이 오면, 입력에 직접 노출되지 않는 은닉층으로. 가장 단순한 신경망 구조는 은닉층에 뉴런이 하나 있는 형태.
     - 세 개 이상 많은 은닉층을 가지면 심층 신경망DNN이라고 한다. 더 많은 연산이 필요하다.
     - 출력층은 한 개의 값이나 벡터값을 출력함.
   - 뉴런 가중치
      - 단위 간 연결 강도를 나타냄. 입력이 출력에 미치는 영향을 수치화.
      - 가중치 0은 입력의 변화가 출력에 영향 없는 상태, 음의 가중치는 입력 증가가 출력에는 감소를 의미.
 - 훈련
  - 훈련이란 모든 가중치를 적합하게 조정해나가는 것. 순전파와 역전파 단계를 반복.
  - 순전파
   - 입력값 받고 예측값이라는 출력을 얻는 과정. 입력 받을 때는 어떠한 연산도 없음.
   - 다음 층에서는 입력값에 곱셈, 덧셈, 활성화 연산을 하고 결과를 다음 층에 전달.
   - 순차적으로 진행 후 마지막 충에서 출력값 도출.
  - 역전파
   - 예측값과 기댓값의 차이를 손실함수(비용함수) J(w)로 변환. w는 가중치.
   - 목표는 훈련셋을 가지고 손실함수를 최적화하는 것. 즉, 손실을 가능한 최소화하는 것.
   - 이 때 사용하는 방법이 경사하강법. 경사가 낮아지는 방향으로 이동하여 가장 낮은 값이 될 때까지.
   - J(w)의 경사를 찾아야 한다. p q r을 각각의 층이라 하면 J(w)=r(q(p()))이다.
   - 첫 층의 가중치의 경사는 두, 세번째 경사에 의존. 두 번째 층의 경사는 세번째 층 가중치의 경사에 의존함.
   - 마지막인 세번째 층부터 역방향 미분을 적용해 나간다. 따라서 역전파이다.
   - 모델 오차를 망을 통해 한 번에 하나의 층으로 전파해 나가며 오차만큼 가중치를 튜닝해나간다.
   - 역전파는 경사를 찾는 가장 명확하고 효율적인 방법이다.
  - 하이퍼파라미터
    - 훈련 이전 정하는 변수라 훈련 중에 바뀌지 않는 변수.
    - 하이퍼파라미터를 통해 신경망을 유연하게 하지만, 이 유연성 때문에 모델 튜닝이 어려워짐.
  - 은닉층 및 노드의 수
   - 은닉층과 노드가 많아지면 매개변수도 많아지고, 모델은 복잡한 함수 구현에 적합해짐.
   - 잘 일반화할 수 있는 훈련된 망 가지려면 은닉층과 은닉층 노드 개수 최적화가 필요.
   - 층 수가 적으면 시스템 오차 증가, 층 수가 많으면 과적합. 그러나 결정하는 확실한 방법은 없다.

     - 대규모 이미지, 음성 인식 같은 복잡한 작업은 수십개 층과 엄청 많은 훈련 데이터 필요.
     - 대부분의 문제에서는 한두개 은닉층으로 시작해서 과적합될 때까지 층의 개수 늘려나감.
     - 숨겨진 노드 수는 입출력 노드 수, 훈련 데이터 양, 모델에서의 함수 복잡도와 관련.
     - 경험적으로, 각 층 숨겨진 노드는 입출력 크기 사이의 중간값으로 정하면 됨.
     - 숨겨진 노드는 과적합 방지를 위해 입력 노드 2배를 넘기면 안 됨.
   - 학습률
     - 가중치 최적화를 위해 순전파, 역전파를 반복한다.
     - 반복마다 각 가중치에 대한 손실함수 미분을 계산하고, 미분값을 가중치에서 뺀다.
     - 얼마나 빠르게/느리게 가중치 값을 변하게 할지는 학습률이 ㄱ려정.
     - 적당한 시간 안에 수렴할 만큼 학습률은 커야 하고, 손실함수 최솟값을 찾을만큼 작아야 한다.
   - 활성화 함수
        - 기대하는 출력을 얻기 위해 입력 가중치 합을 받는 함수이다.
        - 활성화함수를 이용해 신경망은 더 복잡한 방법으로 입력 결합 가능.
        - 또한 신경망 모델과 그 출력의 관계를 더 잘 표현 가능. 어떤 뉴런을 활성화할지, 즉 어떤 정보를 다음 층에 전달할지 결정한다.
        - 활성화함수가 없다면 인공신경망은 큰 학습 능력을 잃게 된다.
            - 선형함수(항등함수)
                - 직선 방정식(f(x)=mx + c)로 나타내고 활성은 입력에 비례한다.
                - 여러 층이 있고 모두 선형적이면, 마지막 층 활성화함수는 첫 번째 층 선형 함수와 같다.
                - 선형 함수 범위는 -inf에서 +inf.
            - 시그모이드 함수
                - S자 형의 함수. 출력 범위는 0~1. 논리 활성화함수 라고도 한다.
                - f(x) = 1 / (1 + e^-x)
            - 하이퍼볼릭 탄젠트 함수(Tanh)
                - Tanh(x) = 2Sigmoid(2x) - 1
                - -1~+1까지의 출력 범위.
            - 렐루 함수 ReLU: 정류선형유닛 Rectified Linear Unit의 줄임말
                - f(x) = max(x, 0)
                - 양수면 입력값을 그대로 전달, 음수면 0을 전달. 함수의 단순성 때문에 많이 활용.
        - 활성화함수를 고르는 방법은 없다.
        - 단지 문제 속성과 모델로 표현되는 관계에 의존할 뿐.
        - 여러 다른 활성화 함수를 적용해보고 더 빨리 수렴하고 효율적 훈련을 보여주는 함수로 선택할 수도 있다.
        - 출력층에서의 활성화함수는 대부분 모델로 표현되는 문제 유형으로 결정됨.
    - 비용 함수(손실 함수)
        - 인공신경망 성능의 정도와 얼마나 실험 데이터에 적합한지를 측정한다.
            - 평균 제곱 오차 MSE
                - 회귀형 문제에 우선적으로 적용. 연속값을 출력한다.
                - 예측과 실제 차이에 제곱하여 평균 낸 것.
            - 교차 엔트로피 (로그 손실)
                - 분류형 문제에 우선적으로 적용.
                - 0~1 사이의 확률값을 출력된다.
                - 예측되는 확률이 실제 분류에서 멀어지면 교차 엔트로피 손실이 커진다. 완벽한 모델은 0의 교차 엔트로피를 갖는 것.
    - 최적화 알고리즘
        - 손실함수를 최소화하는 방향으로 가중치 매개변수를 갱신한다.
        - 비용함수는 최적화 알고리즘에 지역을 안내하는 역할, 전역 최솟값에 도달하기 위해 올바른 방향으로 가고 있는지 알려주는 것.
            - 모멘텀
                - 이전 경사를 현재 단계에서 참조함.
                - 가중치의 이전 갱신이 현재 갱신과 같은 방향으로 간다면(모멘텀이 붙는다면) 더 큰 단계를 선택.
                - 경사 방향이 서로 상반되면 더 작은 단계를 선택
                - 골짜기에 공을 굴린다고 생각했을 때 골짜기 바닥에 가까워질수록 모멘텀이 붙는다는 것.
            - 에이다그레이드 (조정적 경사 알고리즘) Ada Grad
                - 학습률을 매개변수에 맞게 변화시킴. 자주 발생하는 특성들과 관련된 매개변수는 작게 갱신하고 드물게 발생하는 특성들과 관련된 매개변수는 크게 갱신함.
            - 알엠에스프랍 (RMS Prop)
                - 제곱 평균 제곱근 전파의 줄임말.
                - 학습률이 자동으로 변하고 각 매개변수의 학습률이 다르다.
            - 아담 (조정적 순간 예측)
                - 에이다그레이드와 알엠에스프랍 알고리즘을 결합해 최적화를 수행함. 인기가 가장 많은 경사하강 최적화 알고리즘임.
         
        - 에폭
            - 전체 훈련 데이터셋에 대해 신경망을 갱산하는 주기를 에폭이라 함.
            - 데이터 크기와 계산적 제한에 따라 수십, 수백, 수천 번의 에폭으로 신경망을 훈련함.
        - 배치 크기
            - 정방향/역방향 전달로 훈련하는 예제의 횟수.
            - 배치 크기 32는 모델 가중치 갱신 전에 훈련셋으로부터 32개 샘플 사용해 오차 경사를 예측한다는 뜻.
            - 배치 크기가 클수록 메모리 공간 많이 필요.
     
### 3.2. 인공신경망 모델 생성
- 케라스 및 머신러닝 패키지 설치
    - 사용자 친화적인 딥러닝 파이썬 라이브러리.
    - 텐서플로와 테아노 같은 복잡한 수치 계산 엔진을 감싸는 wrapper에 불과하다.
    - 그래서 먼저 텐서플로와 테아노를 설치해야 함.
```python
# 케라스의 패키지 가져오기
from Keras.models import Sequential
from Keras.layers import Dense
import numpy as np

# 데이터 불러오기
# 넘파이 모듈로 임의의 데이터와 분류 생성하기.
# (1000, 10) 크기 데이터 정렬을 먼저 만들고, 0과 1로 이루어진 (1000, 1)  크기의 분류 정렬을 만든다.
data = np.random.random((1000, 10))
Y = np.random.randint(2, size=(1000, 1))
model = Sequential()
```
- 모델 생성 - 신경망 구조
    - 선형적으로 층을 쌓는 케라스 시퀀셜 모델 사용.
    - 시퀀셜 모델 생성하고, 망 구조 완성될 때까지 한 번에 하나씩 층을 추가해 나감.
        - 입력층 입력 개수가 적당한지 가장 먼저 확인.
        - 첫 번째 층 생성할 때 입력 개수 지정.
        - 그 다음 인수 input_dim을 사용해 입력층을 다루고 있음을 나타내기 위해 밀집층dense layer 혹은 완전 연결층을 선택한다.
        - add() 함수로 모델에 층 하나 추가하고 각 층의 노드 수를 지정한다.
        - 출력층으로 또 다른 밀집층 하나 추가한다.
    - 구현하려는 모델 구조는 다음과 같음.
        - 모델에는 10개의 변수가 있다. input_dim=10 인수
        - 첫 은닉층에는 노드 32개, relu 활성화함수.
        - 두 번째 은닉층은 노드 32개, relu 활성화 함수
        - 출력층은 노드 1개, sigmoid 활성화 함수.
```python
model = Sequential()
model.add(Dense(32, input_dim=10, activation= 'relu'))
model.add(Dense(32, activation = 'relu' ))
model.add(Dense(1, activation = 'sigmoid'))
```
- 모델 컴파일
    - compile() 함수로 모델 컴파일 가능.
    - 모델 컴파일이란, 테아노나 텐서플로 패키지에 있는 수치 계산 라이브러리 이용하겠다는 것.
    - 컴파일에서는 신경망 훈련에 필요한 추가 속성을 지정하는 것이 중요.
    - 하나의 신경망 훈련이라는 것은, 문제에 대해 실제 예측하는 가중치의 최적 집합을 찾는 과정이다.
    - 따라서 가중치 집합 평가를 위한 손실함수, 신경망에 대한 다른 가중치 찾으려는 최적화 알고리즘, 훈련 중 필요한 매크릭 지정해야 함.
        - 다음 예제는 cross-entropy 손실 함수 사용. 케라스에서는 binary_crossentropy이다.
        - 아담 최적화 알고리즘 사용하고, 분류형 문제이기 때문에 매트릭으로 분류 정확도도 출력하는 코드이다.
```python
model.compile(loss= 'binary_corssentropy', optimizer = 'adam' , \
    metrics = ['accuracy'])
```
- 모델 적합화
    - fit() 함수를 호출하여 모델 훈련하거나 데이터에 적합화 할 수 있다.
    - 훈련 과정은 nb_epoch를 이용해 지정된 반복 횟수(에포크) 동안 실행됨.
    - 신경망에서 가중치가 갱신되기 전에 인수 batch_size를 이용해 배치 크기 정할 수 있음.
    - 이 변수들은 시행착오를 통해 실험적으로 정할 수 있다.
```python
model.fit(data, Y, nb_epoch=10, batch_size=32)
```
- 모델 평가
    - evaluation() 함수로 훈련셋에 대해 모델 평가 가능.
    - 이 함수는 각 입력, 출력 쌍에 대해 예측하고, 평균손실, 정확도 같이 설정된 매트릭을 포함해 점수를 기록한다.
```python
scores = model.evaluate(X_test, Y_test)
print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
```
- 더 빠른 인공 신경망 모델 실행: GPU와 클라우드 서비스
    - 거대한 계산 능력이 필요
        - CPU는 로컬 머신에서 명령어 처리와 실행을 책임짐.
        - CPU는 코어가 제한적이고 일을 순차적으로 처리하여 딥러닝 훈련에 필요한 행렬 계산을 신속하게 처리하지 못함.
        - 3가지 방법이 있다.
            - GPU가 있는 컴퓨터에서 실행
            - 캐글 커널 또는 구글 코랩에서 실행
            - AWS 이용
- 그래픽 처리 장치 GPU
    - 수천 개 스레드를 동시에 처리할 수 있는 수백 개의 코어로 구성 됨.
    - 특히 복잡한 행렬 연산 처리에 적합함.
- 클라우드 서비스
    - 캐글: 구글이 소유한 데이터 공학 웹사이트. 주피터 서비스를 제공하는데 캐글 커널이라고 함.
        - 무료 서비스이고 흔히 사용하는 패키지가 설치되어 있다.
        - 커널을 캐글 데이터 셋과 연결하거나, 진행 과정 중 새로운 데이터 셋 업로드도 가능함.
    - 구글 코랩: 주피터 노트북 환경에서 GPU 무료로 사용 가능.
    - 아마존 웹 서비스: ASW 딥러닝은 어떤 규모든 클라우드에서 딥러닝 가속화 가능한 하부구조 제공.
        - AWS 서버 인스턴스를 빠르게 실행 가능.
        - 인기 있는 딥러닝 프레임워크가 이미 설치되어 있고, 사용자 정의 인공지능 모델을 훈련하고 새로운 알고리즘 실험하며
        - 새로운 기술, 방법을 배우는데 필요한 인터페이스를 제공함.
        - AWS 서버는 캐글 커널보다 더 오래 실행 가능하다. 대규모 프로젝트라면 AWS를 추천.
- 맺음말
    - 인공신경망은 머신러닝 모든 유형에 사용되는 알고리즘의 집합체.
    - 동물 뉴런과 뉴런층이 있는 생물학적 신경망에 영감을 받음.
    - 다수의 층이 있다면 심층 신경망.
## 4장. 지도학습: 모델 및 개념


















