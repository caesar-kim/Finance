# 3부. 비지도 학습
## 7장. 비지도 학습: 차원 축소 (p.233)

## 8장. 비지도 학습: 군집화 (p.281)

# 4부. 강화학습과 자연어 처리
## 9장. 강화학습 (p.331)
- 경험과 그에 따른 보상이나 패널티에서 배우는 것이 강화학습.
- 보상을 극대화, 패널티는 최소화 하는 정책을 통해 최선의 조치를 찾도록 훈련시키는 방법.
- 불확실하고 역동적인 환경에서 수익률을 극대화하는 에이전트의 개념이 금융 환경과 맞아 알고리즘 거래에 특히 적합함.
- 예측을 생성하지 않고, 시장 구조를 암시적으로 학습하지 않는다.
  - 지속 변화하는 시장에서 포트폴리오 배분 가중치를 동적으로 변경하는 정책을 직접 학습한다.
  - 시행착오를 통해 학습하여 최적의 실행 경로를 스스로 파악한다.

### 9.1. 강화학습: 이론 및 개념 (p.333)
- 에이전트: 동작 수행 본체
- 동작: 에이전트가 환경에서 수행할 수 있는 동작
- 환경: 에이전트가 속해 있는 세계
- 상태: 현재 상황
- 보상: 에이전트가 마지막으로 수행한 동작 평가를 위해 환경에서 보낸 즉각적인 반환
- 강화학습의 목표
  - 실험적 시도와 간단한 피드백에 대한 루프를 통해 최적 전략 학습.
  - 에이전트는 최적 전략으로 환경에 적극 적응하여 보상을 극대화 한다.
  - 에이전트의 수행 결과로 보상 신호를 반환한다.
  - 에이전트와 환경 간 상호작용에는 시간에 따른 일련의 동작과 관찰된 보상이 포함된다.
  - 에이전트는 환경에 대한 지식을 축적하고 최적 정책을 학습하며 최상의 정책을 효율적으로 학습하기 위해 다음에 취할 조치를 결정한다.
- 정책
  - 정책은 에이전트가 결정 내리는 방법을 설명하는 알고리즘 또는 규칙의 집합.
  - 정책은 pi로 표시되는 함수로 상태s와 동작alpha를 매핑한다.
  - alpha_t = pi (s_t)
  - 즉, 현재 상태에 따라 행동을 결정.
  - 정책은 결정적이거나 확률적일 수 있다. 결정적 정책은 상태를 동작에 매핑하고, 확률적 정책은 동작에 대한 확률 분포를 출력한다.
  - 동작 alpha를 취하는 대신 상태가 주어진 동작에 할당된 확률이 있음을 의미한다.
  - 강화학습의 목표는 pi* 라고 하는 최적 정책을 학습하는 것.
- 가치 함수
  - 에이전트 목표는 환경에서 동작 잘 수행하는 방법 학습하기.
  - 수학적으로는 미래 보상 또는 누적 할인 보상 G를 최대화하는 것.

- 실습 링크.
[7.2. 포트폴리오 관리](https://colab.research.google.com/drive/1EUtPBACNv6InEi4Q6MH0j4zDsrfySnU2)  
[9.1. 강화학습 기반 거래 전략](https://colab.research.google.com/drive/11vI2Cs-eoHFGBDWLJtCwRkjwhr0KoCGs)
- 9.1.은 RAM 이슈로 작동 실패하여, 하단에 남깁니다.
